#!/usr/bin/env node
/**
 * Bundle Unity Catalog shim (catalog.py + sql.py)
 * Requires pyspark-shim.py to be loaded first
 */

const fs = require('fs');
const path = require('path');

// Read the Unity Catalog shim files
const catalogShim = fs.readFileSync(
  path.join(__dirname, '../src/shims/pyspark/catalog.py'),
  'utf8'
);

const sqlShim = fs.readFileSync(
  path.join(__dirname, '../src/shims/pyspark/sql.py'),
  'utf8'
);

// Clean module docstrings and imports
const cleanCatalog = catalogShim
  .replace(/"""[\s\S]*?"""\n\n/, '') // Remove module docstring
  .replace(/^import pandas as pd\n/m, '')
  .replace(/^from datetime import.*\n/gm, '')
  .replace(/^from \.core import.*\n/gm, '')
  .replace(/^from \.catalog import.*\n/gm, '');

const cleanSQL = sqlShim
  .replace(/"""[\s\S]*?"""\n\n/, '')
  .replace(/^import pandas as pd\n/m, '')
  .replace(/^import re\n/m, '')
  .replace(/^from \.catalog import.*\n/gm, '')
  .replace(/^from \.core import.*\n/gm, '');

// Bundle everything together
const bundledShim = `# Unity Catalog Shim - Complete bundled version
# Auto-generated by scripts/bundle-unity-catalog.js
# Requires pyspark-shim.py to be loaded first

import pandas as pd
import re
from datetime import datetime

# ============ CATALOG MODULE ============
${cleanCatalog}

# ============ SQL MODULE ============
${cleanSQL}

# ============ REGISTER CATALOG FUNCTIONS ============
# Make catalog functions available in pyspark.sql
import sys
pyspark_sql = sys.modules.get('pyspark.sql')
if pyspark_sql:
    pyspark_sql.current_catalog = current_catalog
    pyspark_sql.current_database = current_database

print("✓ Unity Catalog shim loaded (complete bundle)")
`;

// Write to public/shims
const outputPath = path.join(__dirname, '../public/shims/unity-catalog-shim.py');
fs.writeFileSync(outputPath, bundledShim);

console.log(`✓ Bundled Unity Catalog shim written to ${outputPath}`);
console.log(`  Size: ${Math.round(bundledShim.length / 1024)}KB`);
