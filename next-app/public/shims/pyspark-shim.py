# PySpark Shim - Complete bundled version
# Auto-generated by scripts/bundle-shim.js
import pandas as pd
import sys
from types import ModuleType

# ============ CORE CLASSES ============

# ============ ROW CLASS ============
class Row(dict):
    """PySpark-like Row class"""
    def __init__(self, **kwargs):
        super().__init__(kwargs)
        self.__dict__.update(kwargs)

    def __getattr__(self, name):
        try:
            return self[name]
        except KeyError:
            raise AttributeError(f"'Row' object has no attribute '{name}'")

    def asDict(self):
        return dict(self)


# ============ COLUMN CLASS ============
class Column:
    """PySpark-like Column class for expressions"""
    def __init__(self, name: str, expr: Optional[str] = None):
        self.name = name
        self.expr = expr or name
        self._alias = None

    # Comparison operators
    def __gt__(self, other):
        return Column(self.name, f"({self.expr}) > {repr(other)}")

    def __lt__(self, other):
        return Column(self.name, f"({self.expr}) < {repr(other)}")

    def __ge__(self, other):
        return Column(self.name, f"({self.expr}) >= {repr(other)}")

    def __le__(self, other):
        return Column(self.name, f"({self.expr}) <= {repr(other)}")

    def __eq__(self, other):
        if other is None:
            return Column(self.name, f"({self.expr}).isna()")
        return Column(self.name, f"({self.expr}) == {repr(other)}")

    def __ne__(self, other):
        if other is None:
            return Column(self.name, f"({self.expr}).notna()")
        return Column(self.name, f"({self.expr}) != {repr(other)}")

    # Arithmetic operators
    def __mul__(self, other):
        if isinstance(other, Column):
            return Column(self.name, f"({self.expr}) * ({other.expr})")
        return Column(self.name, f"({self.expr}) * {repr(other)}")

    def __add__(self, other):
        if isinstance(other, Column):
            return Column(self.name, f"({self.expr}) + ({other.expr})")
        return Column(self.name, f"({self.expr}) + {repr(other)}")

    def __sub__(self, other):
        if isinstance(other, Column):
            return Column(self.name, f"({self.expr}) - ({other.expr})")
        return Column(self.name, f"({self.expr}) - {repr(other)}")

    def __truediv__(self, other):
        if isinstance(other, Column):
            return Column(self.name, f"({self.expr}) / ({other.expr})")
        return Column(self.name, f"({self.expr}) / {repr(other)}")

    # Logical operators
    def __and__(self, other):
        if isinstance(other, Column):
            return Column(self.name, f"({self.expr}) & ({other.expr})")
        return Column(self.name, f"({self.expr}) & {repr(other)}")

    def __or__(self, other):
        if isinstance(other, Column):
            return Column(self.name, f"({self.expr}) | ({other.expr})")
        return Column(self.name, f"({self.expr}) | {repr(other)}")

    # Column methods
    def alias(self, name):
        new_col = Column(self.name, self.expr)
        new_col._alias = name
        # Copy over any special attributes
        for attr in ['_agg_func', '_source_col', '_round_decimals', '_transform_func',
                     '_is_window_func', '_window', '_sort_desc']:
            if hasattr(self, attr):
                setattr(new_col, attr, getattr(self, attr))
        return new_col

    def cast(self, dataType):
        # For our simple shim, we'll just return a column with cast info
        new_col = Column(self.name, f"({self.expr}).astype('{dataType}')")
        return new_col

    def over(self, window):
        new_col = Column(self.name, self.expr)
        new_col._window = window
        new_col._is_window_func = True
        # Copy over aggregation info if present
        for attr in ['_agg_func', '_source_col', '_window_func', '_window_args']:
            if hasattr(self, attr):
                setattr(new_col, attr, getattr(self, attr))
        return new_col

    def isNull(self):
        return Column(self.name, f"({self.expr}).isna()")

    def isNotNull(self):
        return Column(self.name, f"({self.expr}).notna()")

    def desc(self):
        new_col = Column(self.name, self.expr)
        new_col._sort_desc = True
        return new_col

    def asc(self):
        new_col = Column(self.name, self.expr)
        new_col._sort_desc = False
        return new_col


# ============ GROUPED DATA ============
class GroupedData:
    """Result of DataFrame.groupBy()"""
    def __init__(self, df, group_cols):
        self._df = df
        self._group_cols = group_cols

    def agg(self, *exprs):
        pdf = self._df._pdf.copy()
        grouped = pdf.groupby(self._group_cols, as_index=False)

        agg_results = {}
        for expr in exprs:
            if hasattr(expr, '_agg_func'):
                col_name = expr._alias or expr.name
                source_col = expr._source_col
                func = expr._agg_func

                if func == 'avg':
                    agg_results[col_name] = grouped[source_col].mean()[source_col]
                elif func == 'sum':
                    agg_results[col_name] = grouped[source_col].sum()[source_col]
                elif func == 'count':
                    agg_results[col_name] = grouped[source_col].count()[source_col]
                elif func == 'min':
                    agg_results[col_name] = grouped[source_col].min()[source_col]
                elif func == 'max':
                    agg_results[col_name] = grouped[source_col].max()[source_col]

                if hasattr(expr, '_round_decimals'):
                    agg_results[col_name] = agg_results[col_name].round(expr._round_decimals)

        result_pdf = grouped[self._group_cols].first()
        for col_name, values in agg_results.items():
            result_pdf[col_name] = values.values

        return DataFrame(result_pdf)

    def pivot(self, pivot_col):
        # Simple pivot implementation
        from .dataframe import PivotBuilder
        return PivotBuilder(self._df, self._group_cols, pivot_col)


# ============ DATAFRAME CLASS ============
class DataFrame:
    """PySpark-like DataFrame backed by pandas"""
    def __init__(self, pdf):
        self._pdf = pdf.copy()

    @property
    def columns(self):
        return list(self._pdf.columns)

    @property
    def write(self):
        from .io import DataFrameWriter
        return DataFrameWriter(self)

    def count(self):
        return len(self._pdf)

    def show(self, n=20, truncate=True):
        print(self._pdf.head(n).to_string())

    def collect(self):
        rows = []
        for _, row in self._pdf.iterrows():
            row_dict = {k: (None if pd.isna(v) else v) for k, v in row.items()}
            rows.append(Row(**row_dict))
        return rows

    def first(self):
        if len(self._pdf) == 0:
            return None
        row = self._pdf.iloc[0]
        return Row(**{k: (None if pd.isna(v) else v) for k, v in row.items()})

    def select(self, *cols):
        result_cols = {}
        for c in cols:
            if isinstance(c, str):
                result_cols[c] = self._pdf[c]
            elif isinstance(c, Column):
                col_name = c._alias or c.name
                if hasattr(c, '_transform_func'):
                    result_cols[col_name] = c._transform_func(self._pdf)
                else:
                    try:
                        result_cols[col_name] = self._pdf.eval(c.expr)
                    except:
                        result_cols[col_name] = self._pdf[c.name]
        return DataFrame(pd.DataFrame(result_cols))

    def filter(self, condition):
        if isinstance(condition, Column):
            mask = self._pdf.eval(condition.expr)
            return DataFrame(self._pdf[mask].reset_index(drop=True))
        return self

    def where(self, condition):
        return self.filter(condition)

    def withColumn(self, name, col):
        pdf = self._pdf.copy()

        # Handle window functions
        if hasattr(col, '_is_window_func') and col._is_window_func:
            window = col._window
            if window._order_cols:
                sort_cols = [oc if isinstance(oc, str) else oc.name for oc in window._order_cols]
                pdf = pdf.sort_values(sort_cols).reset_index(drop=True)

            if hasattr(col, '_agg_func') and col._agg_func == 'sum':
                source_col = col._source_col
                if window._partition_cols:
                    pdf[name] = pdf.groupby(window._partition_cols)[source_col].cumsum()
                else:
                    pdf[name] = pdf[source_col].cumsum()
            return DataFrame(pdf)

        # Handle transform functions
        if hasattr(col, '_transform_func'):
            pdf[name] = col._transform_func(pdf)
            return DataFrame(pdf)

        # Handle expressions
        try:
            pdf[name] = pdf.eval(col.expr)
        except:
            if col.name in pdf.columns:
                pdf[name] = pdf[col.name]
        return DataFrame(pdf)

    def withColumnRenamed(self, existing, new):
        pdf = self._pdf.copy()
        pdf = pdf.rename(columns={existing: new})
        return DataFrame(pdf)

    def groupBy(self, *cols):
        col_names = [c if isinstance(c, str) else c.name for c in cols]
        return GroupedData(self, col_names)

    def agg(self, *exprs):
        # Aggregation without grouping
        result = {}
        for expr in exprs:
            if hasattr(expr, '_agg_func'):
                col_name = expr._alias or expr.name
                source_col = expr._source_col
                func = expr._agg_func

                if func == 'avg':
                    result[col_name] = [self._pdf[source_col].mean()]
                elif func == 'sum':
                    result[col_name] = [self._pdf[source_col].sum()]
                elif func == 'count':
                    result[col_name] = [self._pdf[source_col].count()]
                elif func == 'min':
                    result[col_name] = [self._pdf[source_col].min()]
                elif func == 'max':
                    result[col_name] = [self._pdf[source_col].max()]

                if hasattr(expr, '_round_decimals'):
                    result[col_name] = [round(result[col_name][0], expr._round_decimals)]

        return DataFrame(pd.DataFrame(result))

    def join(self, other, on, how='inner'):
        result = self._pdf.merge(other._pdf, on=on, how=how)
        return DataFrame(result)

    def orderBy(self, *cols):
        sort_cols = []
        asc_list = []
        for c in cols:
            if isinstance(c, str):
                sort_cols.append(c)
                asc_list.append(True)
            else:
                sort_cols.append(c.name)
                asc_list.append(not getattr(c, '_sort_desc', False))
        return DataFrame(self._pdf.sort_values(sort_cols, ascending=asc_list).reset_index(drop=True))

    def drop(self, *cols):
        col_names = [c if isinstance(c, str) else c.name for c in cols]
        return DataFrame(self._pdf.drop(columns=col_names))

    def distinct(self):
        return DataFrame(self._pdf.drop_duplicates().reset_index(drop=True))

    def dropDuplicates(self, subset=None):
        return DataFrame(self._pdf.drop_duplicates(subset=subset).reset_index(drop=True))

    def limit(self, n):
        return DataFrame(self._pdf.head(n).copy())

    def union(self, other):
        result = pd.concat([self._pdf, other._pdf], ignore_index=True)
        return DataFrame(result)

    def fillna(self, value, subset=None):
        pdf = self._pdf.copy()
        if subset:
            pdf[subset] = pdf[subset].fillna(value)
        else:
            pdf = pdf.fillna(value)
        return DataFrame(pdf)

    def dropna(self, how='any', subset=None):
        pdf = self._pdf.copy()
        if subset:
            pdf = pdf.dropna(how=how, subset=subset)
        else:
            pdf = pdf.dropna(how=how)
        return DataFrame(pdf.reset_index(drop=True))

    def toPandas(self):
        return self._pdf.copy()


# ============ SPARK SESSION ============
class SparkSession:
    """Simplified SparkSession for browser environment"""
    @property
    def read(self):
        from .io import DataFrameReader
        return DataFrameReader(self)

    def createDataFrame(self, data, schema):
        pdf = pd.DataFrame(data, columns=schema)
        return DataFrame(pdf)

    def sql(self, sqlQuery: str):
        """
        Execute SQL query and return DataFrame.
        Supports Unity Catalog DDL/DQL/ACL commands.
        """
        from .sql import execute_sql
        return execute_sql(sqlQuery, self)

    def table(self, tableName: str):
        """
        Read table by name from Unity Catalog.
        Name can be 'table', 'schema.table', or 'catalog.schema.table'
        """
        from .catalog import CatalogManager
        return CatalogManager.get_table(tableName)


# Create global spark session
spark = SparkSession()


# ============ FUNCTIONS ============


# ============ COLUMN REFERENCE ============
def col(name):
    """Reference a column by name"""
    return Column(name)


def lit(value):
    """Create a literal value column"""
    c = Column("_lit", repr(value))
    c._lit_value = value

    def transform(pdf):
        return pd.Series([value] * len(pdf))

    c._transform_func = transform
    return c


# ============ AGGREGATE FUNCTIONS ============
def avg(col_name):
    """Calculate average"""
    c = Column(col_name)
    c._agg_func = 'avg'
    c._source_col = col_name
    return c


def sum(col_name):
    """Calculate sum"""
    c = Column(col_name)
    c._agg_func = 'sum'
    c._source_col = col_name
    return c


def count(col_name):
    """Count values"""
    c = Column(col_name)
    c._agg_func = 'count'
    c._source_col = col_name
    return c


def min(col_name):
    """Find minimum value"""
    c = Column(col_name)
    c._agg_func = 'min'
    c._source_col = col_name
    return c


def max(col_name):
    """Find maximum value"""
    c = Column(col_name)
    c._agg_func = 'max'
    c._source_col = col_name
    return c


def round(col_expr, decimals=0):
    """Round numeric values"""
    new_col = Column(col_expr.name, col_expr.expr)
    if hasattr(col_expr, '_agg_func'):
        new_col._agg_func = col_expr._agg_func
        new_col._source_col = col_expr._source_col
    new_col._round_decimals = decimals
    if hasattr(col_expr, '_alias'):
        new_col._alias = col_expr._alias
    return new_col


# ============ STRING FUNCTIONS ============
def upper(col_expr):
    """Convert string to uppercase"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.upper()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.upper()")
    new_col._transform_func = transform
    return new_col


def lower(col_expr):
    """Convert string to lowercase"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.lower()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.lower()")
    new_col._transform_func = transform
    return new_col


def initcap(col_expr):
    """Convert string to title case"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.title()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.title()")
    new_col._transform_func = transform
    return new_col


def concat(*cols):
    """Concatenate multiple columns"""
    def transform(pdf):
        result = pdf[cols[0].name].astype(str)
        for c in cols[1:]:
            if hasattr(c, '_lit_value'):
                result = result + str(c._lit_value)
            else:
                result = result + pdf[c.name].astype(str)
        return result

    new_col = Column("concat_result")
    new_col._transform_func = transform
    return new_col


def concat_ws(sep, *cols):
    """Concatenate with separator"""
    def transform(pdf):
        string_cols = []
        for c in cols:
            if hasattr(c, 'name'):
                string_cols.append(pdf[c.name].astype(str))
            else:
                string_cols.append(pd.Series([str(c)] * len(pdf)))
        return pd.Series([sep.join(row) for row in zip(*string_cols)])

    new_col = Column("concat_ws_result")
    new_col._transform_func = transform
    return new_col


def length(col_expr):
    """Get string length"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.len()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.len()")
    new_col._transform_func = transform
    return new_col


def substring(col_expr, pos, length):
    """Extract substring (1-indexed!)"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        # PySpark substring is 1-indexed, Python is 0-indexed
        return pdf[col_expr.name].str.slice(pos - 1, pos - 1 + length)

    new_col = Column(col_expr.name, f"{col_expr.name}.str.slice({pos-1}, {pos-1+length})")
    new_col._transform_func = transform
    return new_col


def trim(col_expr):
    """Trim whitespace from both ends"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.strip()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.strip()")
    new_col._transform_func = transform
    return new_col


def ltrim(col_expr):
    """Trim whitespace from left"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.lstrip()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.lstrip()")
    new_col._transform_func = transform
    return new_col


def rtrim(col_expr):
    """Trim whitespace from right"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.rstrip()

    new_col = Column(col_expr.name, f"{col_expr.name}.str.rstrip()")
    new_col._transform_func = transform
    return new_col


def lpad(col_expr, length, pad):
    """Left pad to length"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.pad(length, side='left', fillchar=pad)

    new_col = Column(col_expr.name)
    new_col._transform_func = transform
    return new_col


def rpad(col_expr, length, pad):
    """Right pad to length"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.pad(length, side='right', fillchar=pad)

    new_col = Column(col_expr.name)
    new_col._transform_func = transform
    return new_col


# ============ CONDITIONAL FUNCTIONS ============
def when(condition, value):
    """Start a conditional expression"""
    return ConditionalColumn(condition, value)


class ConditionalColumn:
    """Builder for when/otherwise expressions"""
    def __init__(self, condition, value):
        self.conditions = [(condition, value)]
        self.otherwise_value = None

    def when(self, condition, value):
        self.conditions.append((condition, value))
        return self

    def otherwise(self, value):
        self.otherwise_value = value
        return self._build_column()

    def _build_column(self):
        def transform(pdf):
            result = pd.Series([self.otherwise_value] * len(pdf))
            for condition, value in reversed(self.conditions):
                mask = pdf.eval(condition.expr)
                result[mask] = value
            return result

        col = Column("when_result")
        col._transform_func = transform
        return col


# ============ ARRAY/COLLECTION FUNCTIONS ============
def explode(col_expr):
    """Explode an array column into multiple rows"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    # This is a special case that needs to be handled at DataFrame level
    # For now, we'll mark it and handle in select()
    new_col = Column(col_expr.name)
    new_col._is_explode = True
    return new_col


def split(col_expr, pattern):
    """Split string into array"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)

    def transform(pdf):
        return pdf[col_expr.name].str.split(pattern)

    new_col = Column(col_expr.name)
    new_col._transform_func = transform
    return new_col


# ============ NULL HANDLING ============
def isnull(col_expr):
    """Check if column is null"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)
    return col_expr.isNull()


def isnan(col_expr):
    """Check if column is NaN"""
    if isinstance(col_expr, str):
        col_expr = col(col_expr)
    return Column(col_expr.name, f"({col_expr.expr}).isna()")


def coalesce(*cols):
    """Return first non-null value"""
    def transform(pdf):
        result = None
        for c in cols:
            if result is None:
                if hasattr(c, 'name'):
                    result = pdf[c.name]
                else:
                    result = pd.Series([c] * len(pdf))
            else:
                mask = result.isna()
                if hasattr(c, 'name'):
                    result[mask] = pdf[c.name][mask]
                else:
                    result[mask] = c
        return result

    new_col = Column("coalesce_result")
    new_col._transform_func = transform
    return new_col


# ============ WINDOW FUNCTIONS ============

class WindowSpec:
    """Window specification for window functions"""
    unboundedPreceding = "unboundedPreceding"
    unboundedFollowing = "unboundedFollowing"
    currentRow = "currentRow"

    def __init__(self):
        self._partition_cols = []
        self._order_cols = []
        self._row_start = None
        self._row_end = None

    def partitionBy(self, *cols):
        new_spec = WindowSpec()
        new_spec._partition_cols = [c if isinstance(c, str) else c.name for c in cols]
        new_spec._order_cols = self._order_cols
        new_spec._row_start = self._row_start
        new_spec._row_end = self._row_end
        return new_spec

    def orderBy(self, *cols):
        new_spec = WindowSpec()
        new_spec._partition_cols = self._partition_cols
        new_spec._order_cols = list(cols)
        new_spec._row_start = self._row_start
        new_spec._row_end = self._row_end
        return new_spec

    def rowsBetween(self, start, end):
        new_spec = WindowSpec()
        new_spec._partition_cols = self._partition_cols
        new_spec._order_cols = self._order_cols
        new_spec._row_start = start
        new_spec._row_end = end
        return new_spec

class Window:
    """Window functions factory"""
    unboundedPreceding = "unboundedPreceding"
    unboundedFollowing = "unboundedFollowing"
    currentRow = "currentRow"

    @staticmethod
    def partitionBy(*cols):
        return WindowSpec().partitionBy(*cols)

    @staticmethod
    def orderBy(*cols):
        return WindowSpec().orderBy(*cols)

# ============ WINDOW FUNCTIONS ============
def row_number():
    """Assign sequential row numbers within partitions"""
    col = Column("row_number")
    col._window_func = 'row_number'
    col._is_window_func = True
    return col

def rank():
    """Assign ranks with gaps"""
    col = Column("rank")
    col._window_func = 'rank'
    col._is_window_func = True
    return col

def dense_rank():
    """Assign ranks without gaps"""
    col = Column("dense_rank")
    col._window_func = 'dense_rank'
    col._is_window_func = True
    return col

def lag(col_name, offset=1, default=None):
    """Access previous row value"""
    col = Column(col_name)
    col._window_func = 'lag'
    col._window_args = {'offset': offset, 'default': default}
    col._is_window_func = True
    return col

def lead(col_name, offset=1, default=None):
    """Access next row value"""
    col = Column(col_name)
    col._window_func = 'lead'
    col._window_args = {'offset': offset, 'default': default}
    col._is_window_func = True
    return col


# ============ I/O CLASSES ============


class DataFrameWriter:
    """Writer interface for saving DataFrames to tables"""

    def __init__(self, df):
        self._df = df
        self._mode = "error"  # error, overwrite, append, ignore
        self._format = "delta"  # delta, parquet, etc. (mostly cosmetic)

    def mode(self, mode: str):
        """
        Set write mode:
        - error: Throw error if table exists (default)
        - overwrite: Overwrite existing table
        - append: Append to existing table
        - ignore: Silently ignore if table exists
        """
        self._mode = mode.lower()
        return self

    def format(self, source: str):
        """Set format (delta, parquet, etc.) - noop in shim"""
        self._format = source.lower()
        return self

    def saveAsTable(self, name: str):
        """
        Save DataFrame as table in Unity Catalog.
        Name can be:
        - 'table' -> saved to current catalog.schema.table
        - 'schema.table' -> saved to current catalog.schema.table
        - 'catalog.schema.table' -> saved with full path
        """
        # Import here to avoid circular dependency
        from .catalog import CatalogManager

        # Check if table exists
        try:
            existing_df = CatalogManager.get_table(name)
            table_exists = True
        except:
            table_exists = False

        # Handle mode logic
        if table_exists:
            if self._mode == "error":
                raise ValueError(f"Table {name} already exists. Use mode('overwrite') or mode('append')")
            elif self._mode == "ignore":
                return  # Do nothing
            elif self._mode == "append":
                # Append data to existing table
                import pandas as pd
                existing_pdf = existing_df._pdf
                combined_pdf = pd.concat([existing_pdf, self._df._pdf], ignore_index=True)
                from .core import DataFrame
                self._df = DataFrame(combined_pdf)
            # overwrite mode continues to register_table below

        # Register the table
        CatalogManager.register_table(name, self._df, is_managed=True)

    def save(self, path: str):
        """Alias for saveAsTable for compatibility"""
        return self.saveAsTable(path)


class DataFrameReader:
    """Reader interface for loading DataFrames from tables"""

    def __init__(self, spark_session):
        self._spark = spark_session
        self._format = "delta"

    def format(self, source: str):
        """Set format (delta, parquet, etc.) - noop in shim"""
        self._format = source.lower()
        return self

    def table(self, name: str):
        """
        Read table by name from Unity Catalog.
        Name can be:
        - 'table' -> reads from current catalog.schema.table
        - 'schema.table' -> reads from current catalog.schema.table
        - 'catalog.schema.table' -> reads with full path
        """
        from .catalog import CatalogManager
        return CatalogManager.get_table(name)

    def load(self, path: str):
        """Alias for table for compatibility"""
        return self.table(path)


# ============ MODULE SETUP ============
# Create proper module structure for imports
pyspark_module = ModuleType('pyspark')
pyspark_sql_module = ModuleType('pyspark.sql')
pyspark_sql_functions_module = ModuleType('pyspark.sql.functions')
pyspark_sql_window_module = ModuleType('pyspark.sql.window')

# Add classes to pyspark.sql
pyspark_sql_module.Row = Row
pyspark_sql_module.Column = Column
pyspark_sql_module.DataFrame = DataFrame
pyspark_sql_module.SparkSession = SparkSession
pyspark_sql_module.GroupedData = GroupedData
pyspark_sql_module.DataFrameWriter = DataFrameWriter
pyspark_sql_module.DataFrameReader = DataFrameReader

# Add all functions to pyspark.sql.functions
pyspark_sql_functions_module.col = col
pyspark_sql_functions_module.lit = lit
pyspark_sql_functions_module.avg = avg
pyspark_sql_functions_module.sum = sum
pyspark_sql_functions_module.count = count
pyspark_sql_functions_module.min = min
pyspark_sql_functions_module.max = max
pyspark_sql_functions_module.round = round
pyspark_sql_functions_module.upper = upper
pyspark_sql_functions_module.lower = lower
pyspark_sql_functions_module.initcap = initcap
pyspark_sql_functions_module.concat = concat
pyspark_sql_functions_module.concat_ws = concat_ws
pyspark_sql_functions_module.length = length
pyspark_sql_functions_module.substring = substring
pyspark_sql_functions_module.trim = trim
pyspark_sql_functions_module.ltrim = ltrim
pyspark_sql_functions_module.rtrim = rtrim
pyspark_sql_functions_module.lpad = lpad
pyspark_sql_functions_module.rpad = rpad
pyspark_sql_functions_module.when = when
pyspark_sql_functions_module.explode = explode
pyspark_sql_functions_module.split = split
pyspark_sql_functions_module.isnull = isnull
pyspark_sql_functions_module.isnan = isnan
pyspark_sql_functions_module.coalesce = coalesce
pyspark_sql_functions_module.row_number = row_number
pyspark_sql_functions_module.rank = rank
pyspark_sql_functions_module.dense_rank = dense_rank
pyspark_sql_functions_module.lag = lag
pyspark_sql_functions_module.lead = lead

# Add window to pyspark.sql.window
pyspark_sql_window_module.Window = Window
pyspark_sql_window_module.WindowSpec = WindowSpec

# Register modules in sys.modules
sys.modules['pyspark'] = pyspark_module
sys.modules['pyspark.sql'] = pyspark_sql_module
sys.modules['pyspark.sql.functions'] = pyspark_sql_functions_module
sys.modules['pyspark.sql.window'] = pyspark_sql_window_module

# Make spark available globally
spark = SparkSession()

print("âœ“ PySpark shim loaded (complete bundle)")
