# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

PySpark Koans is a browser-based, test-driven learning environment for PySpark and Delta Lake. It runs entirely in the browser using Pyodide (Python in WebAssembly) with a pandas-backed PySpark shim that emulates Spark APIs without requiring a real Spark cluster.

## Commands

All commands run from the `next-app/` directory:

```bash
cd next-app
npm run dev      # Start development server
npm run build    # Build for production
npm run lint     # Run ESLint
npm run export   # Build and export static files
```

Regenerate koan index after adding/modifying koans:
```bash
node scripts/generate-index.js
```

## Architecture

### Data Flow
```
User edits code â†’ Run â†’ Pyodide executes Python â†’
PySpark shim intercepts calls â†’ pandas performs computation â†’
Assertions verify correctness â†’ Success/error feedback
```

### Key Components

**PySpark Shim** (`next-app/public/shims/pyspark-shim.py`): ~850 lines of Python that implements PySpark APIs using pandas. Loaded into Pyodide at runtime. Supports:
- DataFrame operations (select, filter, withColumn, join, groupBy, orderBy, etc.)
- Column expressions with arithmetic, comparison, and logical operators
- Functions module (F.sum, F.avg, F.upper, F.when, etc.)
- Window functions (row_number, lag, lead)
- Creates fake `pyspark.sql` module structure in `sys.modules`

**Koan System**: Each koan file exports an object with:
- `id`, `title`, `category`, `difficulty`
- `setup`: Python code run before user code (creates test data)
- `template`: User-editable code with `___` blanks to fill
- `solution`: Reference answer
- `hints`: Array of progressive hints

**Koan Registry** (`next-app/src/koans/index.js`): Auto-generated by `scripts/generate-index.js`. Imports all koan files and provides `getKoan()`, `getAllKoanIds()`, `getKoansByCategory()`.

**usePyodide Hook** (`next-app/src/hooks/usePyodide.js`): Manages Pyodide initialization, loads pandas and shims, exposes `pyodide` instance for executing user code.

### Directory Structure

```
next-app/
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ index.js              # Landing page
â”‚   â”œâ”€â”€ koans/[id].js         # Dynamic koan page (main learning UI)
â”‚   â””â”€â”€ badge.js              # Achievement badge display
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/           # React components (KoanEditor, Sidebar, etc.)
â”‚   â”œâ”€â”€ hooks/                # usePyodide, useKoanProgress
â”‚   â””â”€â”€ koans/                # Koan definitions by category
â”‚       â”œâ”€â”€ pyspark/          # basics/, column-ops/, strings/, aggregations/, joins/, windows/, nulls/, advanced/
â”‚       â”œâ”€â”€ delta/            # Delta Lake koans (101-110)
â”‚       â””â”€â”€ index.js          # Auto-generated registry
â”œâ”€â”€ public/shims/
â”‚   â””â”€â”€ pyspark-shim.py       # Bundled PySpark shim
â””â”€â”€ scripts/
    â”œâ”€â”€ generate-index.js     # Regenerates koan registry
    â””â”€â”€ bundle-shim.js        # Bundles Python shim files
```

## Adding New Koans

1. Create file in appropriate category folder: `next-app/src/koans/pyspark/{category}/koan-{id}-{slug}.js`

2. Export koan object:
```javascript
export default {
  id: 31,
  title: "Your Koan Title",
  category: "Basics",  // Must match existing category or add to getAllCategories()
  difficulty: "beginner",  // beginner, intermediate, advanced
  setup: `data = [...]\ncolumns = [...]`,
  template: `# Instructions\nresult = df.___(___)\nassert result.count() == 5\nprint("ðŸŽ‰ Koan complete!")`,
  solution: `result = df.filter(col("x") > 5)`,
  hints: ["First hint", "Second hint"],
  examCoverage: ["DEA", "DAA"],  // Optional: DEA, DEP, DAA, MLA
};
```

3. Run `node scripts/generate-index.js` to update registry

4. Koan IDs: 1-99 for PySpark, 101+ for Delta Lake

## Extending the Shim

To add new PySpark functionality, edit `next-app/public/shims/pyspark-shim.py`:

1. **DataFrame method**: Add to `DataFrame` class, implement using `self._pdf` (pandas DataFrame)
2. **Column operation**: Add to `Column` class, return new `Column` with expression string
3. **Function**: Add standalone function, return `Column` with `_transform_func` or `_agg_func`
4. **Register**: Add to `pyspark_sql_functions_module.{name} = {name}` at bottom of file

## Koan Categories

- **Basics** (1-7): Creating DataFrames, selecting, filtering, groupBy, dropping columns, distinct
- **Column Operations** (9-12): Renaming, literals, when/otherwise, type casting
- **String Functions** (13-16): Case conversion, concatenation, substring, trim/pad
- **Aggregations** (17-19): groupBy().agg(), multiple aggregations, aggregate without grouping
- **Joins** (20-22): Inner join, left outer join, multi-column joins
- **Window Functions** (23-25): Running totals, row_number, lag/lead
- **Null Handling** (26-27): isNull detection, fillna/dropna
- **Advanced** (28-30): Union, explode arrays, pivot tables
- **Delta Lake** (101-110): Create table, time travel, merge/upsert, history, optimize, delete, update, vacuum

## Delta Lake Shim

The Delta Lake shim (loaded on demand) provides:
- `DeltaTable` class with MERGE, UPDATE, DELETE operations
- Time travel via version snapshots stored in memory
- History tracking for all operations
- OPTIMIZE and VACUUM simulation

## Exam Coverage

Koans are tagged for Databricks certification relevance:
- **DEA** - Data Engineer Associate
- **DEP** - Data Engineer Professional
- **DAA** - Data Analyst Associate
- **MLA** - Machine Learning Associate

## Limitations

- Single-threaded, in-memory only (no distributed semantics)
- No `spark.sql()` support
- No UDFs
- Window functions are limited
- All data must fit in browser memory
