# Databricks Koans - Project Overview

Learn Databricks through test-driven exercises in the browser.

---

## Current State

### Implemented (Fully Mockable in Browser)

| Section | Koans | Status |
|---------|-------|--------|
| **PySpark Basics** | 7 | âœ… Complete |
| **Delta Lake** | 10 | âœ… Complete |

**Total: 17 koans**

### Files

```
pyspark-koans-prototype/
â”œâ”€â”€ pyspark-delta-koans-complete.jsx   # Full React app with both sections
â”œâ”€â”€ pyspark-koans-expanded.jsx         # PySpark only (30 koans)
â”œâ”€â”€ pyspark-koans.jsx                  # Original prototype (7 koans)
â”œâ”€â”€ delta_lake_shim.py                 # Standalone Delta mock (Python)
â”œâ”€â”€ delta_lake_koans.js                # Delta koans as JS module
â””â”€â”€ docs/
    â”œâ”€â”€ pyspark-koans.md               # PySpark koan reference
    â”œâ”€â”€ delta-lake-koans.md            # Delta Lake koan reference
    â””â”€â”€ project-overview.md            # This file
```

---

## Koan Inventory

### PySpark Basics (IDs 1-7)

| ID | Title | Difficulty | Exams |
|----|-------|------------|-------|
| 1 | Creating a DataFrame | ğŸŸ¢ Beginner | DEA, DAA, MLA |
| 2 | Selecting Columns | ğŸŸ¢ Beginner | DEA, DAA, MLA |
| 3 | Filtering Rows | ğŸŸ¢ Beginner | DEA, DAA, MLA |
| 4 | Adding Columns | ğŸŸ¢ Beginner | DEA, DAA, MLA |
| 5 | Grouping and Aggregating | ğŸŸ¡ Intermediate | DEA, DAA, MLA |
| 6 | Joining DataFrames | ğŸŸ¡ Intermediate | DEA, DAA |
| 7 | Window Functions | ğŸŸ¡ Intermediate | DEA, DEP, DAA |

### Delta Lake (IDs 101-110)

| ID | Title | Difficulty | Exams |
|----|-------|------------|-------|
| 101 | Creating a Delta Table | ğŸŸ¢ Beginner | DEA, DAA |
| 102 | Time Travel - Version | ğŸŸ¢ Beginner | DEA, DEP, DAA |
| 103 | MERGE - Upsert Pattern | ğŸŸ¡ Intermediate | DEA, DEP |
| 104 | MERGE - Selective Update | ğŸŸ¡ Intermediate | DEP |
| 105 | Table History | ğŸŸ¢ Beginner | DEA, DEP, DAA |
| 106 | OPTIMIZE and Z-ORDER | ğŸŸ¡ Intermediate | DEA, DEP |
| 107 | Delete with Condition | ğŸŸ¢ Beginner | DEA, DEP |
| 108 | Update with Condition | ğŸŸ¢ Beginner | DEA, DEP |
| 109 | Create Table with Builder | ğŸŸ¡ Intermediate | DEP |
| 110 | VACUUM Old Files | ğŸŸ¡ Intermediate | DEA, DEP |

---

## Planned Sections

### Fully Mockable (Browser Only)

| Section | Est. Koans | Priority | Notes |
|---------|------------|----------|-------|
| PySpark - String Functions | 8 | High | upper, lower, concat, substring, etc. |
| PySpark - Date/Time Functions | 8 | High | date_add, datediff, extraction |
| PySpark - Null Handling | 5 | High | isNull, fillna, coalesce |
| PySpark - Complex Types | 8 | Medium | arrays, maps, explode |
| Spark SQL - Basic Queries | 10 | High | SELECT, WHERE, GROUP BY |
| Spark SQL - Joins & CTEs | 8 | Medium | JOIN syntax, WITH clauses |
| MLflow - Experiment Tracking | 8 | Medium | log_param, log_metric |
| MLflow - Model Registry | 6 | Medium | register_model, stages |
| DABs - YAML Structure | 8 | Medium | bundle.yml validation |

### Conceptual/Simulated (Parked for Now)

| Section | Est. Koans | Notes |
|---------|------------|-------|
| Unity Catalog | 10 | Could use UC OSS for real interaction |
| Structured Streaming | 10 | Hard to mock properly |
| Delta Live Tables | 8 | Decorator patterns only |
| Auto Loader | 5 | Conceptual only |
| Spark ML | 15 | Some parts mockable |

---

## Architecture

### Current (Browser-Only)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Browser                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  React UI                                               â”‚
â”‚  â”œâ”€â”€ Sidebar (categories, progress)                     â”‚
â”‚  â”œâ”€â”€ Code editor (textarea)                             â”‚
â”‚  â””â”€â”€ Output panel                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Pyodide (Python in WebAssembly)                        â”‚
â”‚  â”œâ”€â”€ pandas (real library)                              â”‚
â”‚  â”œâ”€â”€ PySpark Shim (~400 lines)                          â”‚
â”‚  â”‚   â”œâ”€â”€ SparkSession                                   â”‚
â”‚  â”‚   â”œâ”€â”€ DataFrame                                      â”‚
â”‚  â”‚   â”œâ”€â”€ Column expressions                             â”‚
â”‚  â”‚   â”œâ”€â”€ Window functions                               â”‚
â”‚  â”‚   â””â”€â”€ Aggregations                                   â”‚
â”‚  â””â”€â”€ Delta Lake Shim (~300 lines)                       â”‚
â”‚      â”œâ”€â”€ DeltaTable                                     â”‚
â”‚      â”œâ”€â”€ MergeBuilder                                   â”‚
â”‚      â”œâ”€â”€ Time travel (version snapshots)                â”‚
â”‚      â””â”€â”€ History tracking                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Future (Optional Real Backend)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Browser                                    â”‚
â”‚  â”œâ”€â”€ Mode: [Simulated / Local UC / Hosted]                     â”‚
â”‚  â””â”€â”€ Pyodide + Shims (or real API calls)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ REST API (when "Real" mode)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Unity Catalog OSS (Optional)                    â”‚
â”‚  â”œâ”€â”€ Real catalog/schema/table metadata                        â”‚
â”‚  â”œâ”€â”€ Real GRANT/REVOKE                                         â”‚
â”‚  â””â”€â”€ PostgreSQL backend                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Running Locally

```bash
# Create Vite project
npm create vite@latest databricks-koans -- --template react

# Enter directory
cd databricks-koans

# Install dependencies
npm install

# Copy pyspark-delta-koans-complete.jsx content to src/App.jsx

# Run dev server
npm run dev

# Open http://localhost:5173
```

---

## Exam Coverage Summary

| Exam | Code | Current Coverage | Target |
|------|------|------------------|--------|
| Data Engineer Associate | DEA | 14/17 koans | 40+ |
| Data Engineer Professional | DEP | 9/17 koans | 60+ |
| Data Analyst Associate | DAA | 10/17 koans | 30+ |
| ML Associate | MLA | 5/17 koans | 25+ |
| ML Professional | MLP | 0/17 koans | 20+ |

---

## Next Steps

1. **Expand PySpark** - Add string, date, null handling koans
2. **Add Spark SQL** - Build SQL parser/executor for DDL/DML
3. **Add MLflow** - Mock experiment tracking and model registry
4. **UI Improvements** - Exam filters, Monaco editor, persistence
5. **Deployment** - Set up on Vercel, custom domain

---

## References

- [Delta Lake Documentation](https://docs.delta.io/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Unity Catalog OSS](https://github.com/unitycatalog/unitycatalog)
- [Databricks Certifications](https://www.databricks.com/learn/certification)
