{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Koans - Practice Notebook\n",
        "\n",
        "This notebook contains all 59 koans as exercises. Fill in the blanks marked with `___` to complete each koan.\n",
        "\n",
        "**Note**: These koans are designed to work with the browser-based pandas shim. To run with real PySpark, you'll need a Spark environment.\n",
        "\n",
        "## Categories:\n",
        "- **Koans 1-30**: PySpark Basics and Operations\n",
        "- **Koans 101-110**: Delta Lake\n",
        "- **Koans 201-210**: Unity Catalog\n",
        "- **Koans 301-310**: Pandas API on Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "# For browser-based version, spark is already initialized\n",
        "# This notebook assumes you have PySpark available\n",
        "\n",
        "print(\"âœ“ Environment ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 1: Creating a DataFrame\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
        "columns = [\"name\", \"age\"]\n",
        "\n",
        "# Exercise: Create a DataFrame from the data and columns\n",
        "df = spark.___(data, columns)\n",
        "\n",
        "# The DataFrame should have 3 rows\n",
        "assert df.count() == 3, f\"Expected 3 rows, got {df.count()}\"\n",
        "print(\"âœ“ DataFrame created with correct row count\")\n",
        "\n",
        "# The DataFrame should have 2 columns\n",
        "assert len(df.columns) == 2, f\"Expected 2 columns, got {len(df.columns)}\"\n",
        "print(\"âœ“ DataFrame has correct number of columns\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to create a DataFrame.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 2: Selecting Columns\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34, \"NYC\"), (\"Bob\", 45, \"LA\"), (\"Charlie\", 29, \"Chicago\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\n",
        "\n",
        "# Exercise: Select only the 'name' and 'city' columns\n",
        "result = df.___(\"name\", \"___\")\n",
        "\n",
        "# Result should have exactly 2 columns\n",
        "assert len(result.columns) == 2, f\"Expected 2 columns, got {len(result.columns)}\"\n",
        "print(\"âœ“ Correct number of columns selected\")\n",
        "\n",
        "# Result should contain 'name' and 'city'\n",
        "assert \"name\" in result.columns, \"Missing 'name' column\"\n",
        "assert \"city\" in result.columns, \"Missing 'city' column\"\n",
        "print(\"âœ“ Correct columns selected\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to select columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 3: Filtering Rows\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29), (\"Diana\", 52)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Filter to only include people over 35\n",
        "result = df.___(col(\"age\") ___ 35)\n",
        "\n",
        "# Should have 2 people over 35\n",
        "assert result.count() == 2, f\"Expected 2 rows, got {result.count()}\"\n",
        "print(\"âœ“ Correct number of rows filtered\")\n",
        "\n",
        "# Collect and verify\n",
        "rows = result.collect()\n",
        "ages = [row[\"age\"] for row in rows]\n",
        "assert all(age > 35 for age in ages), \"Some ages are not > 35\"\n",
        "print(\"âœ“ All remaining rows have age > 35\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to filter rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 4: Adding Columns\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Add a new column 'age_in_months' that multiplies age by 12\n",
        "result = df.___(\"age_in_months\", col(\"___\") * 12)\n",
        "\n",
        "# Should still have 3 rows\n",
        "assert result.count() == 3\n",
        "print(\"âœ“ Row count unchanged\")\n",
        "\n",
        "# Should now have 3 columns\n",
        "assert len(result.columns) == 3, f\"Expected 3 columns, got {len(result.columns)}\"\n",
        "print(\"âœ“ New column added\")\n",
        "\n",
        "# Check calculation is correct\n",
        "first_row = result.filter(col(\"name\") == \"Alice\").collect()[0]\n",
        "assert first_row[\"age_in_months\"] == 408, f\"Expected 408, got {first_row['age_in_months']}\"\n",
        "print(\"âœ“ Calculation is correct (34 * 12 = 408)\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to add columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 5: Grouping and Aggregating\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 5000),\n",
        "    (\"Sales\", \"Bob\", 4500),\n",
        "    (\"Engineering\", \"Charlie\", 6000),\n",
        "    (\"Engineering\", \"Diana\", 6500),\n",
        "    (\"Engineering\", \"Eve\", 5500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"])\n",
        "\n",
        "# Exercise: Group by department and calculate average salary\n",
        "result = df.___(\"department\").agg(\n",
        "    round(___(\"salary\"), 2).alias(\"avg_salary\")\n",
        ")\n",
        "\n",
        "# Should have 2 departments\n",
        "assert result.count() == 2, f\"Expected 2 groups, got {result.count()}\"\n",
        "print(\"âœ“ Correct number of groups\")\n",
        "\n",
        "# Check Engineering average (6000 + 6500 + 5500) / 3 = 6000\n",
        "eng_row = result.filter(col(\"department\") == \"Engineering\").collect()[0]\n",
        "assert eng_row[\"avg_salary\"] == 6000.0, f\"Expected 6000.0, got {eng_row['avg_salary']}\"\n",
        "print(\"âœ“ Engineering average salary is correct\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to group and aggregate.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 6: Dropping Columns\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34, \"NYC\", \"F\"), (\"Bob\", 45, \"LA\", \"M\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"gender\"])\n",
        "\n",
        "# Exercise: Drop the 'gender' column\n",
        "result = df.___(\"gender\")\n",
        "\n",
        "assert \"gender\" not in result.columns, \"gender column should be dropped\"\n",
        "assert len(result.columns) == 3, f\"Expected 3 columns, got {len(result.columns)}\"\n",
        "print(\"âœ“ Dropped gender column\")\n",
        "\n",
        "# Drop multiple columns\n",
        "result2 = df.___(\"city\", \"gender\")\n",
        "assert len(result2.columns) == 2, f\"Expected 2 columns, got {len(result2.columns)}\"\n",
        "print(\"âœ“ Dropped multiple columns\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to drop columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 7: Distinct Values\n",
        "# Category: Basics\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", \"NYC\"), (\"Bob\", \"LA\"), (\"Alice\", \"NYC\"), (\"Charlie\", \"NYC\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"city\"])\n",
        "\n",
        "# Exercise: Get distinct rows\n",
        "result = df.___()\n",
        "\n",
        "assert result.count() == 3, f\"Expected 3 distinct rows, got {result.count()}\"\n",
        "print(\"âœ“ Got distinct rows\")\n",
        "\n",
        "# Get distinct cities only\n",
        "cities = df.select(\"city\").___()\n",
        "assert cities.count() == 2, f\"Expected 2 distinct cities, got {cities.count()}\"\n",
        "print(\"âœ“ Got distinct cities (NYC, LA)\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to get distinct values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 9: Renaming Columns\n",
        "# Category: Column Operations\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Rename 'name' to 'employee_name'\n",
        "result = df.___(___, \"employee_name\")\n",
        "\n",
        "assert \"employee_name\" in result.columns, \"Should have employee_name column\"\n",
        "assert \"name\" not in result.columns, \"Should not have name column anymore\"\n",
        "print(\"âœ“ Renamed name to employee_name\")\n",
        "\n",
        "# Rename using alias in select\n",
        "result2 = df.select(col(\"name\").___(\"full_name\"), col(\"age\"))\n",
        "assert \"full_name\" in result2.columns, \"Should have full_name column\"\n",
        "print(\"âœ“ Used alias in select\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to rename columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 10: Literal Values\n",
        "# Category: Column Operations\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Add a column 'country' with value 'USA' for all rows\n",
        "result = df.withColumn(\"country\", ___(\"USA\"))\n",
        "\n",
        "rows = result.collect()\n",
        "assert all(row[\"country\"] == \"USA\" for row in rows), \"All rows should have country=USA\"\n",
        "print(\"âœ“ Added literal column\")\n",
        "\n",
        "# Add a numeric literal\n",
        "result2 = df.withColumn(\"bonus\", ___(1000))\n",
        "assert result2.collect()[0][\"bonus\"] == 1000, \"Bonus should be 1000\"\n",
        "print(\"âœ“ Added numeric literal\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to use literal values.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 11: Conditional Logic with when/otherwise\n",
        "# Category: Column Operations\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 17), (\"Diana\", 65)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Create an 'age_group' column based on age\n",
        "result = df.withColumn(\n",
        "    \"age_group\",\n",
        "    ___(col(\"age\") < 18, \"minor\")\n",
        "    .when(col(\"age\") < 65, \"adult\")\n",
        "    ._____(\"senior\")\n",
        ")\n",
        "\n",
        "rows = result.collect()\n",
        "groups = {row[\"name\"]: row[\"age_group\"] for row in rows}\n",
        "\n",
        "assert groups[\"Charlie\"] == \"minor\", f\"Charlie should be minor, got {groups['Charlie']}\"\n",
        "print(\"âœ“ Charlie (17) is minor\")\n",
        "\n",
        "assert groups[\"Alice\"] == \"adult\", f\"Alice should be adult, got {groups['Alice']}\"\n",
        "print(\"âœ“ Alice (34) is adult\")\n",
        "\n",
        "assert groups[\"Diana\"] == \"senior\", f\"Diana should be senior, got {groups['Diana']}\"\n",
        "print(\"âœ“ Diana (65) is senior\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned conditional column logic.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 12: Type Casting\n",
        "# Category: Column Operations\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", \"34\"), (\"Bob\", \"45\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age_str\"])\n",
        "\n",
        "# Exercise: Cast age_str from string to integer\n",
        "result = df.withColumn(\"age\", col(\"age_str\").cast(\"___ \"))\n",
        "\n",
        "# Verify we can do math on the new column\n",
        "result = result.withColumn(\"age_plus_10\", col(\"age\") + 10)\n",
        "\n",
        "rows = result.collect()\n",
        "assert rows[0][\"age_plus_10\"] == 44, f\"Expected 44, got {rows[0]['age_plus_10']}\"\n",
        "print(\"âœ“ Cast to integer and performed math\")\n",
        "\n",
        "# Cast to double\n",
        "result2 = df.withColumn(\"age_float\", col(\"age_str\").cast(\"___ \"))\n",
        "print(\"âœ“ Cast to double\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to cast types.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 13: String Functions - Case\n",
        "# Category: String Functions\n",
        "\n",
        "# Setup\n",
        "data = [(\"alice smith\",), (\"BOB JONES\",), (\"Charlie Brown\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "# Exercise: Convert to uppercase\n",
        "result = df.withColumn(\"upper_name\", ___(col(\"name\")))\n",
        "assert result.collect()[0][\"upper_name\"] == \"ALICE SMITH\"\n",
        "print(\"âœ“ Converted to uppercase\")\n",
        "\n",
        "# Convert to lowercase\n",
        "result = df.withColumn(\"lower_name\", ___(col(\"name\")))\n",
        "assert result.collect()[1][\"lower_name\"] == \"bob jones\"\n",
        "print(\"âœ“ Converted to lowercase\")\n",
        "\n",
        "# Convert to title case (capitalize first letter of each word)\n",
        "result = df.withColumn(\"title_name\", ___(col(\"name\")))\n",
        "assert result.collect()[0][\"title_name\"] == \"Alice Smith\"\n",
        "print(\"âœ“ Converted to title case\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned string case functions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 14: String Functions - Concatenation\n",
        "# Category: String Functions\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", \"Smith\"), (\"Bob\", \"Jones\")]\n",
        "df = spark.createDataFrame(data, [\"first\", \"last\"])\n",
        "\n",
        "# Exercise: Concatenate first and last name with a space\n",
        "result = df.withColumn(\"full_name\", ___(col(\"first\"), lit(\" \"), col(\"last\")))\n",
        "assert result.collect()[0][\"full_name\"] == \"Alice Smith\"\n",
        "print(\"âœ“ Concatenated with concat()\")\n",
        "\n",
        "# Use concat_ws (with separator) - cleaner for multiple values\n",
        "result2 = df.withColumn(\"full_name\", ___(\" \", col(\"first\"), col(\"last\")))\n",
        "assert result2.collect()[0][\"full_name\"] == \"Alice Smith\"\n",
        "print(\"âœ“ Concatenated with concat_ws()\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned string concatenation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 15: String Functions - Substring and Length\n",
        "# Category: String Functions\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\",), (\"Bob\",), (\"Charlotte\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "# Exercise: Get the length of each name\n",
        "result = df.withColumn(\"name_length\", ___(col(\"name\")))\n",
        "lengths = [row[\"name_length\"] for row in result.collect()]\n",
        "assert lengths == [5, 3, 9], f\"Expected [5, 3, 9], got {lengths}\"\n",
        "print(\"âœ“ Calculated string lengths\")\n",
        "\n",
        "# Get first 3 characters (substring is 1-indexed!)\n",
        "result2 = df.withColumn(\"first_three\", ___(col(\"name\"), 1, 3))\n",
        "firsts = [row[\"first_three\"] for row in result2.collect()]\n",
        "assert firsts == [\"Ali\", \"Bob\", \"Cha\"], f\"Expected ['Ali', 'Bob', 'Cha'], got {firsts}\"\n",
        "print(\"âœ“ Extracted first 3 characters\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned substring and length.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 16: String Functions - Trim and Pad\n",
        "# Category: String Functions\n",
        "\n",
        "# Setup\n",
        "data = [(\"  Alice  \",), (\"Bob\",), (\" Charlie \",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "# Exercise: Trim whitespace from both sides\n",
        "result = df.withColumn(\"trimmed\", ___(col(\"name\")))\n",
        "trimmed = [row[\"trimmed\"] for row in result.collect()]\n",
        "assert trimmed == [\"Alice\", \"Bob\", \"Charlie\"], f\"Expected trimmed names, got {trimmed}\"\n",
        "print(\"âœ“ Trimmed whitespace\")\n",
        "\n",
        "# Pad names to 10 characters with asterisks\n",
        "result2 = df.withColumn(\"trimmed\", trim(col(\"name\")))\n",
        "result2 = result2.withColumn(\"padded\", ___(col(\"trimmed\"), 10, \"*\"))\n",
        "assert result2.collect()[1][\"padded\"] == \"*******Bob\"\n",
        "print(\"âœ“ Left-padded with asterisks\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned trim and pad functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 17: Grouping and Aggregating\n",
        "# Category: Aggregations\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 5000),\n",
        "    (\"Sales\", \"Bob\", 4500),\n",
        "    (\"Engineering\", \"Charlie\", 6000),\n",
        "    (\"Engineering\", \"Diana\", 6500),\n",
        "    (\"Engineering\", \"Eve\", 5500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"])\n",
        "\n",
        "# Exercise: Group by department and calculate average salary\n",
        "result = df.___(\"department\").agg(\n",
        "    round(___(\"salary\"), 2).alias(\"avg_salary\")\n",
        ")\n",
        "\n",
        "# Should have 2 departments\n",
        "assert result.count() == 2, f\"Expected 2 groups, got {result.count()}\"\n",
        "print(\"âœ“ Correct number of groups\")\n",
        "\n",
        "# Check Engineering average (6000 + 6500 + 5500) / 3 = 6000\n",
        "eng_row = result.filter(col(\"department\") == \"Engineering\").collect()[0]\n",
        "assert eng_row[\"avg_salary\"] == 6000.0, f\"Expected 6000.0, got {eng_row['avg_salary']}\"\n",
        "print(\"âœ“ Engineering average salary is correct\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to group and aggregate.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 18: Multiple Aggregations\n",
        "# Category: Aggregations\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"Sales\", 5000), (\"Sales\", 4500), (\"Sales\", 6000),\n",
        "    (\"Engineering\", 6000), (\"Engineering\", 6500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"department\", \"salary\"])\n",
        "\n",
        "# Exercise: Calculate min, max, avg, and count per department\n",
        "result = df.groupBy(\"department\").agg(\n",
        "    ___(\"salary\").alias(\"min_salary\"),\n",
        "    ___(\"salary\").alias(\"max_salary\"),\n",
        "    avg(\"salary\").alias(\"avg_salary\"),\n",
        "    ___(\"salary\").alias(\"emp_count\")\n",
        ")\n",
        "\n",
        "sales = result.filter(col(\"department\") == \"Sales\").collect()[0]\n",
        "\n",
        "assert sales[\"min_salary\"] == 4500, f\"Min should be 4500, got {sales['min_salary']}\"\n",
        "print(\"âœ“ Min salary correct\")\n",
        "\n",
        "assert sales[\"max_salary\"] == 6000, f\"Max should be 6000, got {sales['max_salary']}\"\n",
        "print(\"âœ“ Max salary correct\")\n",
        "\n",
        "assert sales[\"emp_count\"] == 3, f\"Count should be 3, got {sales['emp_count']}\"\n",
        "print(\"âœ“ Employee count correct\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned multiple aggregations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 19: Aggregate Without Grouping\n",
        "# Category: Aggregations\n",
        "\n",
        "# Setup\n",
        "data = [(100,), (200,), (300,), (400,), (500,)]\n",
        "df = spark.createDataFrame(data, [\"value\"])\n",
        "\n",
        "# Exercise: Calculate sum of all values without grouping\n",
        "result = df.___(spark_sum(\"value\").alias(\"total\"))\n",
        "\n",
        "total = result.collect()[0][\"total\"]\n",
        "assert total == 1500, f\"Expected 1500, got {total}\"\n",
        "print(\"âœ“ Sum calculated: 1500\")\n",
        "\n",
        "# Calculate multiple aggregates\n",
        "result2 = df.agg(\n",
        "    spark_sum(\"value\").alias(\"total\"),\n",
        "    ___(\"value\").alias(\"average\"),\n",
        "    count(\"value\").alias(\"num_rows\")\n",
        ")\n",
        "\n",
        "row = result2.collect()[0]\n",
        "assert row[\"average\"] == 300.0, f\"Expected 300.0, got {row['average']}\"\n",
        "assert row[\"num_rows\"] == 5, f\"Expected 5, got {row['num_rows']}\"\n",
        "print(\"âœ“ Multiple aggregates calculated\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned global aggregations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 20: Inner Join\n",
        "# Category: Joins\n",
        "\n",
        "# Setup\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 101),\n",
        "    (2, \"Bob\", 102),\n",
        "    (3, \"Charlie\", 101)\n",
        "], [\"emp_id\", \"name\", \"dept_id\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (101, \"Engineering\"),\n",
        "    (102, \"Sales\"),\n",
        "    (103, \"Marketing\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "# Exercise: Join employees with departments on dept_id\n",
        "result = employees.___(departments, ___, \"inner\")\n",
        "\n",
        "# Should have 3 rows (all employees have matching departments)\n",
        "assert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\n",
        "print(\"âœ“ Correct number of joined rows\")\n",
        "\n",
        "# Should have columns from both DataFrames\n",
        "assert \"name\" in result.columns, \"Missing 'name' column\"\n",
        "assert \"dept_name\" in result.columns, \"Missing 'dept_name' column\"\n",
        "print(\"âœ“ Columns from both DataFrames present\")\n",
        "\n",
        "# Alice should be in Engineering\n",
        "alice = result.filter(col(\"name\") == \"Alice\").collect()[0]\n",
        "assert alice[\"dept_name\"] == \"Engineering\", f\"Expected Engineering, got {alice['dept_name']}\"\n",
        "print(\"âœ“ Join matched correctly\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned inner joins.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 21: Left Outer Join\n",
        "# Category: Joins\n",
        "\n",
        "# Setup\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 101),\n",
        "    (2, \"Bob\", 102),\n",
        "    (3, \"Charlie\", 999)  # No matching department!\n",
        "], [\"emp_id\", \"name\", \"dept_id\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (101, \"Engineering\"),\n",
        "    (102, \"Sales\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "# Exercise: Left join to keep all employees, even without matching dept\n",
        "result = employees.join(departments, \"dept_id\", \"___\")\n",
        "\n",
        "# Should have 3 rows (all employees kept)\n",
        "assert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\n",
        "print(\"âœ“ All employees kept\")\n",
        "\n",
        "# Charlie should have null department name\n",
        "charlie = result.filter(col(\"name\") == \"Charlie\").collect()[0]\n",
        "assert charlie[\"dept_name\"] is None, f\"Expected None, got {charlie['dept_name']}\"\n",
        "print(\"âœ“ Charlie has no matching department (null)\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned left outer joins.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 22: Join on Multiple Columns\n",
        "# Category: Joins\n",
        "\n",
        "# Setup\n",
        "orders = spark.createDataFrame([\n",
        "    (\"2024\", \"Q1\", \"Alice\", 100),\n",
        "    (\"2024\", \"Q2\", \"Alice\", 150),\n",
        "    (\"2024\", \"Q1\", \"Bob\", 200)\n",
        "], [\"year\", \"quarter\", \"rep\", \"amount\"])\n",
        "\n",
        "targets = spark.createDataFrame([\n",
        "    (\"2024\", \"Q1\", 120),\n",
        "    (\"2024\", \"Q2\", 140)\n",
        "], [\"year\", \"quarter\", \"target\"])\n",
        "\n",
        "# Exercise: Join on both year and quarter\n",
        "result = orders.join(targets, [___, ___], \"inner\")\n",
        "\n",
        "# Should have 3 rows\n",
        "assert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\n",
        "print(\"âœ“ Joined on multiple columns\")\n",
        "\n",
        "# Check that Alice Q1 has target 120\n",
        "alice_q1 = result.filter((col(\"rep\") == \"Alice\") & (col(\"quarter\") == \"Q1\")).collect()[0]\n",
        "assert alice_q1[\"target\"] == 120, f\"Expected target 120, got {alice_q1['target']}\"\n",
        "print(\"âœ“ Targets matched correctly\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned multi-column joins.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 23: Window Functions - Running Total\n",
        "# Category: Window Functions\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"2024-01-01\", 100),\n",
        "    (\"2024-01-02\", 150),\n",
        "    (\"2024-01-03\", 200),\n",
        "    (\"2024-01-04\", 175)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"date\", \"sales\"])\n",
        "\n",
        "# Exercise: Create a window that orders by date and includes all previous rows\n",
        "window_spec = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.___)\n",
        "\n",
        "# Add running total column\n",
        "result = df.withColumn(\"running_total\", ___(\"sales\").over(window_spec))\n",
        "\n",
        "# Check the running totals\n",
        "rows = result.orderBy(\"date\").collect()\n",
        "\n",
        "assert rows[0][\"running_total\"] == 100, \"Day 1 should be 100\"\n",
        "print(\"âœ“ Day 1: 100\")\n",
        "\n",
        "assert rows[1][\"running_total\"] == 250, \"Day 2 should be 250 (100+150)\"\n",
        "print(\"âœ“ Day 2: 250\")\n",
        "\n",
        "assert rows[3][\"running_total\"] == 625, \"Day 4 should be 625\"\n",
        "print(\"âœ“ Day 4: 625 (cumulative)\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned window running totals.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 24: Window Functions - Row Number\n",
        "# Category: Window Functions\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 5000),\n",
        "    (\"Sales\", \"Bob\", 5500),\n",
        "    (\"Engineering\", \"Charlie\", 6000),\n",
        "    (\"Engineering\", \"Diana\", 6500),\n",
        "    (\"Engineering\", \"Eve\", 5500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"dept\", \"name\", \"salary\"])\n",
        "\n",
        "# Exercise: Rank employees within each department by salary (highest first)\n",
        "window_spec = Window.partitionBy(\"___\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "result = df.withColumn(\"rank\", ___().___(window_spec))\n",
        "\n",
        "# Check rankings\n",
        "eng = result.filter(col(\"dept\") == \"Engineering\").orderBy(\"rank\").collect()\n",
        "assert eng[0][\"name\"] == \"Diana\", f\"Diana should be #1 in Engineering, got {eng[0]['name']}\"\n",
        "assert eng[0][\"rank\"] == 1\n",
        "print(\"âœ“ Diana is #1 in Engineering ($6500)\")\n",
        "\n",
        "assert eng[1][\"name\"] == \"Charlie\", f\"Charlie should be #2, got {eng[1]['name']}\"\n",
        "print(\"âœ“ Charlie is #2 in Engineering ($6000)\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned row_number().\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 25: Window Functions - Lag and Lead\n",
        "# Category: Window Functions\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"2024-01-01\", 100),\n",
        "    (\"2024-01-02\", 150),\n",
        "    (\"2024-01-03\", 120),\n",
        "    (\"2024-01-04\", 200)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"date\", \"price\"])\n",
        "\n",
        "# Exercise: Get yesterday's price and calculate daily change\n",
        "window_spec = Window.orderBy(\"date\")\n",
        "\n",
        "result = df.withColumn(\"prev_price\", ___(\"price\", 1).over(window_spec))\n",
        "result = result.withColumn(\"change\", col(\"price\") - col(\"prev_price\"))\n",
        "\n",
        "rows = result.orderBy(\"date\").collect()\n",
        "\n",
        "# First row has no previous\n",
        "assert rows[0][\"prev_price\"] is None, \"First row should have no prev_price\"\n",
        "print(\"âœ“ First row has no previous\")\n",
        "\n",
        "# Second row: prev=100, change=50\n",
        "assert rows[1][\"prev_price\"] == 100, f\"Expected prev=100, got {rows[1]['prev_price']}\"\n",
        "assert rows[1][\"change\"] == 50, f\"Expected change=50, got {rows[1]['change']}\"\n",
        "print(\"âœ“ Day 2: prev=100, change=+50\")\n",
        "\n",
        "# Get tomorrow's price\n",
        "result2 = df.withColumn(\"next_price\", ___(\"price\", 1).over(window_spec))\n",
        "rows2 = result2.orderBy(\"date\").collect()\n",
        "assert rows2[0][\"next_price\"] == 150, f\"Expected next=150, got {rows2[0]['next_price']}\"\n",
        "print(\"âœ“ Lead shows next day's price\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned lag and lead.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Null Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 26: Handling Nulls - Detection\n",
        "# Category: Null Handling\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", None), (\"Charlie\", 29), (None, 45)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Filter to rows where age is not null\n",
        "result = df.filter(col(\"age\").___())\n",
        "\n",
        "assert result.count() == 3, f\"Expected 3 rows with age, got {result.count()}\"\n",
        "print(\"âœ“ Filtered to non-null ages\")\n",
        "\n",
        "# Filter to rows where age IS null\n",
        "nulls = df.filter(col(\"age\").___())\n",
        "assert nulls.count() == 1, f\"Expected 1 null age, got {nulls.count()}\"\n",
        "print(\"âœ“ Found rows with null age\")\n",
        "\n",
        "# Check for null name\n",
        "null_names = df.filter(col(\"name\").isNull())\n",
        "assert null_names.count() == 1\n",
        "print(\"âœ“ Found row with null name\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned null detection.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 27: Handling Nulls - Fill and Drop\n",
        "# Category: Null Handling\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", 34), (\"Bob\", None), (None, 29), (\"Diana\", None)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Fill null ages with 0\n",
        "result = df.___(0, subset=[\"age\"])\n",
        "\n",
        "ages = [row[\"age\"] for row in result.collect()]\n",
        "assert None not in ages, \"Should have no null ages\"\n",
        "assert ages.count(0) == 2, \"Should have 2 zeros\"\n",
        "print(\"âœ“ Filled null ages with 0\")\n",
        "\n",
        "# Fill null names with \"Unknown\"\n",
        "result2 = df.fillna(\"Unknown\", subset=[\"name\"])\n",
        "names = [row[\"name\"] for row in result2.collect()]\n",
        "assert \"Unknown\" in names, \"Should have Unknown name\"\n",
        "print(\"âœ“ Filled null names\")\n",
        "\n",
        "# Drop rows with ANY null values\n",
        "result3 = df.___()\n",
        "assert result3.count() == 1, f\"Expected 1 complete row, got {result3.count()}\"\n",
        "print(\"âœ“ Dropped rows with nulls\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to handle nulls.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 28: Union DataFrames\n",
        "# Category: Advanced\n",
        "\n",
        "# Setup\n",
        "df1 = spark.createDataFrame([(\"Alice\", 34), (\"Bob\", 45)], [\"name\", \"age\"])\n",
        "df2 = spark.createDataFrame([(\"Charlie\", 29), (\"Diana\", 52)], [\"name\", \"age\"])\n",
        "\n",
        "# Exercise: Combine two DataFrames with the same schema\n",
        "result = df1.___(df2)\n",
        "\n",
        "assert result.count() == 4, f\"Expected 4 rows, got {result.count()}\"\n",
        "print(\"âœ“ Combined DataFrames\")\n",
        "\n",
        "names = [row[\"name\"] for row in result.collect()]\n",
        "assert \"Alice\" in names and \"Charlie\" in names, \"Should have names from both DFs\"\n",
        "print(\"âœ“ Contains data from both DataFrames\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to union DataFrames.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 29: Explode Arrays\n",
        "# Category: Advanced\n",
        "\n",
        "# Setup\n",
        "data = [(\"Alice\", \"python,sql,spark\"), (\"Bob\", \"java,scala\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"skills_str\"])\n",
        "\n",
        "# First split the string into an array\n",
        "df = df.withColumn(\"skills\", split(col(\"skills_str\"), \",\"))\n",
        "\n",
        "# Exercise: Explode the skills array into separate rows\n",
        "result = df.select(\"name\", ___(col(\"skills\")).alias(\"skill\"))\n",
        "\n",
        "assert result.count() == 5, f\"Expected 5 rows, got {result.count()}\"\n",
        "print(\"âœ“ Exploded to 5 rows\")\n",
        "\n",
        "alice_skills = [row[\"skill\"] for row in result.filter(col(\"name\") == \"Alice\").collect()]\n",
        "assert len(alice_skills) == 3, f\"Alice should have 3 skills, got {len(alice_skills)}\"\n",
        "assert \"spark\" in alice_skills\n",
        "print(\"âœ“ Alice has 3 skills including spark\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned to explode arrays.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 30: Pivot Tables\n",
        "# Category: Advanced\n",
        "\n",
        "# Setup\n",
        "data = [\n",
        "    (\"Alice\", \"Q1\", 100), (\"Alice\", \"Q2\", 150),\n",
        "    (\"Bob\", \"Q1\", 200), (\"Bob\", \"Q2\", 180)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"name\", \"quarter\", \"sales\"])\n",
        "\n",
        "# Exercise: Pivot to get quarters as columns\n",
        "result = df.groupBy(\"name\").___(___).agg(spark_sum(\"sales\"))\n",
        "\n",
        "# Should have columns: name, Q1, Q2\n",
        "assert \"Q1\" in result.columns, \"Should have Q1 column\"\n",
        "assert \"Q2\" in result.columns, \"Should have Q2 column\"\n",
        "print(\"âœ“ Pivoted quarters to columns\")\n",
        "\n",
        "alice = result.filter(col(\"name\") == \"Alice\").collect()[0]\n",
        "assert alice[\"Q1\"] == 100, f\"Expected Q1=100, got {alice['Q1']}\"\n",
        "assert alice[\"Q2\"] == 150, f\"Expected Q2=150, got {alice['Q2']}\"\n",
        "print(\"âœ“ Values correctly placed in columns\")\n",
        "\n",
        "print(\"\\\\nðŸŽ‰ Koan complete! You've learned pivot tables.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delta Lake, Unity Catalog, and Pandas API on Spark\n",
        "\n",
        "The remaining koans (101-210, 301-310) require specialized environments and are documented in the solutions notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
