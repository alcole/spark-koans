{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Koans - Test Notebook\n",
        "\n",
        "This notebook contains  all 59 koans for testing and verification.\n",
        "\n",
        "**Note**: These koans are designed to work with the browser-based pandas shim. ",
        "To run with real PySpark, you'll need a Spark environment.\n",
        "\n",
        "## Categories:\n",
        "- **Koans 1-30**: PySpark Basics and Operations\n",
        "- **Koans 101-110**: Delta Lake\n",
        "- **Koans 201-210**: Unity Catalog\n",
        "- **Koans 301-310**: Pandas API on Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize PySpark (uncomment if running with real Spark)\n",
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import *\n",
        "# from delta import *\n",
        "#\n",
        "# spark = SparkSession.builder \\\n",
        "#     .appName(\"PySpark Koans\") \\\n",
        "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "#     .getOrCreate()\n",
        "\n",
        "# For browser-based version, spark is already initialized\n",
        "# This notebook assumes you have PySpark available\n",
        "\n",
        "print(\"‚úì Environment ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 1: Creating a DataFrame\n# Category: Basics\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\ncolumns = [\"name\", \"age\"]\n\n# Exercise (fill in the blanks)\n# Create a DataFrame from the data and columns\ndf = spark.___(___, ___)\n\n# The DataFrame should have 3 rows\nassert df.count() == 3, f\"Expected 3 rows, got {df.count()}\"\nprint(\"‚úì DataFrame created with correct row count\")\n\n# The DataFrame should have 2 columns\nassert len(df.columns) == 2, f\"Expected 2 columns, got {len(df.columns)}\"\nprint(\"‚úì DataFrame has correct number of columns\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to create a DataFrame.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 2: Selecting Columns\n# Category: Basics\n\n# Setup\ndata = [(\"Alice\", 34, \"NYC\"), (\"Bob\", 45, \"LA\"), (\"Charlie\", 29, \"Chicago\")]\ndf = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\n\n# Exercise (fill in the blanks)\n# Select only the 'name' and 'city' columns\nresult = df.___(\"name\", \"___\")\n\n# Result should have exactly 2 columns\nassert len(result.columns) == 2, f\"Expected 2 columns, got {len(result.columns)}\"\nprint(\"‚úì Correct number of columns selected\")\n\n# Result should contain 'name' and 'city'\nassert \"name\" in result.columns, \"Missing 'name' column\"\nassert \"city\" in result.columns, \"Missing 'city' column\"\nprint(\"‚úì Correct columns selected\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to select columns.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 3: Filtering Rows\n# Category: Basics\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29), (\"Diana\", 52)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Filter to only include people over 35\nfrom pyspark.sql.functions import col\n\nresult = df.___(col(\"age\") ___ 35)\n\n# Should have 2 people over 35\nassert result.count() == 2, f\"Expected 2 rows, got {result.count()}\"\nprint(\"‚úì Correct number of rows filtered\")\n\n# Collect and verify\nrows = result.collect()\nages = [row[\"age\"] for row in rows]\nassert all(age > 35 for age in ages), \"Some ages are not > 35\"\nprint(\"‚úì All remaining rows have age > 35\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to filter rows.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 4: Adding Columns\n# Category: Basics\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Add a new column 'age_in_months' that multiplies age by 12\nfrom pyspark.sql.functions import col\n\nresult = df.___(\"age_in_months\", col(\"___\") * 12)\n\n# Should still have 3 rows\nassert result.count() == 3\nprint(\"‚úì Row count unchanged\")\n\n# Should now have 3 columns\nassert len(result.columns) == 3, f\"Expected 3 columns, got {len(result.columns)}\"\nprint(\"‚úì New column added\")\n\n# Check calculation is correct\nfirst_row = result.filter(col(\"name\") == \"Alice\").collect()[0]\nassert first_row[\"age_in_months\"] == 408, f\"Expected 408, got {first_row['age_in_months']}\"\nprint(\"‚úì Calculation is correct (34 * 12 = 408)\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to add columns.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 5: Grouping and Aggregating\n# Category: Basics\n\n# Setup\ndata = [\n    (\"Sales\", \"Alice\", 5000),\n    (\"Sales\", \"Bob\", 4500),\n    (\"Engineering\", \"Charlie\", 6000),\n    (\"Engineering\", \"Diana\", 6500),\n    (\"Engineering\", \"Eve\", 5500)\n]\ndf = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"])\n\n# Exercise (fill in the blanks)\n# Group by department and calculate average salary\nfrom pyspark.sql.functions import avg, round, col\n\nresult = df.___(\"department\").agg(\n    round(___(\"salary\"), 2).alias(\"avg_salary\")\n)\n\n# Should have 2 departments\nassert result.count() == 2, f\"Expected 2 groups, got {result.count()}\"\nprint(\"‚úì Correct number of groups\")\n\n# Check Engineering average (6000 + 6500 + 5500) / 3 = 6000\neng_row = result.filter(col(\"department\") == \"Engineering\").collect()[0]\nassert eng_row[\"avg_salary\"] == 6000.0, f\"Expected 6000.0, got {eng_row['avg_salary']}\"\nprint(\"‚úì Engineering average salary is correct\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to group and aggregate.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 6: Dropping Columns\n# Category: Basics\n\n# Setup\ndata = [(\"Alice\", 34, \"NYC\", \"F\"), (\"Bob\", 45, \"LA\", \"M\")]\ndf = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"gender\"])\n\n# Exercise (fill in the blanks)\n# Drop the 'gender' column\nresult = df.___(\"gender\")\n\nassert \"gender\" not in result.columns, \"gender column should be dropped\"\nassert len(result.columns) == 3, f\"Expected 3 columns, got {len(result.columns)}\"\nprint(\"‚úì Dropped gender column\")\n\n# Drop multiple columns\nresult2 = df.___(\"city\", \"gender\")\nassert len(result2.columns) == 2, f\"Expected 2 columns, got {len(result2.columns)}\"\nprint(\"‚úì Dropped multiple columns\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to drop columns.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 7: Distinct Values\n# Category: Basics\n\n# Setup\ndata = [(\"Alice\", \"NYC\"), (\"Bob\", \"LA\"), (\"Alice\", \"NYC\"), (\"Charlie\", \"NYC\")]\ndf = spark.createDataFrame(data, [\"name\", \"city\"])\n\n# Exercise (fill in the blanks)\n# Get distinct rows\nresult = df.___()\n\nassert result.count() == 3, f\"Expected 3 distinct rows, got {result.count()}\"\nprint(\"‚úì Got distinct rows\")\n\n# Get distinct cities only\ncities = df.select(\"city\").___()\nassert cities.count() == 2, f\"Expected 2 distinct cities, got {cities.count()}\"\nprint(\"‚úì Got distinct cities (NYC, LA)\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to get distinct values.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 9: Renaming Columns\n# Category: Column Operations\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", 45)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Rename 'name' to 'employee_name'\nfrom pyspark.sql.functions import col\n\nresult = df.___(___, \"employee_name\")\n\nassert \"employee_name\" in result.columns, \"Should have employee_name column\"\nassert \"name\" not in result.columns, \"Should not have name column anymore\"\nprint(\"‚úì Renamed name to employee_name\")\n\n# Rename using alias in select\nresult2 = df.select(col(\"name\").___(\"full_name\"), col(\"age\"))\nassert \"full_name\" in result2.columns, \"Should have full_name column\"\nprint(\"‚úì Used alias in select\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to rename columns.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 10: Literal Values\n# Category: Column Operations\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", 45)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Add a column 'country' with value 'USA' for all rows\nfrom pyspark.sql.functions import lit\n\nresult = df.withColumn(\"country\", ___(\"USA\"))\n\nrows = result.collect()\nassert all(row[\"country\"] == \"USA\" for row in rows), \"All rows should have country=USA\"\nprint(\"‚úì Added literal column\")\n\n# Add a numeric literal\nresult2 = df.withColumn(\"bonus\", ___(1000))\nassert result2.collect()[0][\"bonus\"] == 1000, \"Bonus should be 1000\"\nprint(\"‚úì Added numeric literal\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to use literal values.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 11: Conditional Logic with when/otherwise\n# Category: Column Operations\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 17), (\"Diana\", 65)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Create an 'age_group' column based on age\nfrom pyspark.sql.functions import when, col\n\nresult = df.withColumn(\n    \"age_group\",\n    _____(col(\"age\") < 18, \"minor\")\n    .when(col(\"age\") < 65, \"adult\")\n    ._____(\"senior\")\n)\n\nrows = result.collect()\ngroups = {row[\"name\"]: row[\"age_group\"] for row in rows}\n\nassert groups[\"Charlie\"] == \"minor\", f\"Charlie should be minor, got {groups['Charlie']}\"\nprint(\"‚úì Charlie (17) is minor\")\n\nassert groups[\"Alice\"] == \"adult\", f\"Alice should be adult, got {groups['Alice']}\"\nprint(\"‚úì Alice (34) is adult\")\n\nassert groups[\"Diana\"] == \"senior\", f\"Diana should be senior, got {groups['Diana']}\"\nprint(\"‚úì Diana (65) is senior\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned conditional column logic.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 12: Type Casting\n# Category: Column Operations\n\n# Setup\ndata = [(\"Alice\", \"34\"), (\"Bob\", \"45\")]\ndf = spark.createDataFrame(data, [\"name\", \"age_str\"])\n\n# Exercise (fill in the blanks)\n# Cast age_str from string to integer\nresult = df.withColumn(\"age\", col(\"age_str\").cast(\"___\"))\n\n# Verify we can do math on the new column\nresult = result.withColumn(\"age_plus_10\", col(\"age\") + 10)\n\nrows = result.collect()\nassert rows[0][\"age_plus_10\"] == 44, f\"Expected 44, got {rows[0]['age_plus_10']}\"\nprint(\"‚úì Cast to integer and performed math\")\n\n# Cast to double\nresult2 = df.withColumn(\"age_float\", col(\"age_str\").___(\"double\"))\nprint(\"‚úì Cast to double\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to cast types.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 13: String Functions - Case\n# Category: String Functions\n\n# Setup\ndata = [(\"alice smith\",), (\"BOB JONES\",), (\"Charlie Brown\",)]\ndf = spark.createDataFrame(data, [\"name\"])\n\n# Exercise (fill in the blanks)\n# Convert to uppercase\nfrom pyspark.sql.functions import upper, lower, initcap\n\nresult = df.withColumn(\"upper_name\", ___(col(\"name\")))\nassert result.collect()[0][\"upper_name\"] == \"ALICE SMITH\"\nprint(\"‚úì Converted to uppercase\")\n\n# Convert to lowercase\nresult = df.withColumn(\"lower_name\", ___(col(\"name\")))\nassert result.collect()[1][\"lower_name\"] == \"bob jones\"\nprint(\"‚úì Converted to lowercase\")\n\n# Convert to title case (capitalize first letter of each word)\nresult = df.withColumn(\"title_name\", ___(col(\"name\")))\nassert result.collect()[0][\"title_name\"] == \"Alice Smith\"\nprint(\"‚úì Converted to title case\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned string case functions.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 14: String Functions - Concatenation\n# Category: String Functions\n\n# Setup\ndata = [(\"Alice\", \"Smith\"), (\"Bob\", \"Jones\")]\ndf = spark.createDataFrame(data, [\"first\", \"last\"])\n\n# Exercise (fill in the blanks)\n# Concatenate first and last name with a space\nfrom pyspark.sql.functions import concat, concat_ws, lit\n\nresult = df.withColumn(\"full_name\", ___(col(\"first\"), lit(\" \"), col(\"last\")))\nassert result.collect()[0][\"full_name\"] == \"Alice Smith\"\nprint(\"‚úì Concatenated with concat()\")\n\n# Use concat_ws (with separator) - cleaner for multiple values\nresult2 = df.withColumn(\"full_name\", ___(\" \", col(\"first\"), col(\"last\")))\nassert result2.collect()[0][\"full_name\"] == \"Alice Smith\"\nprint(\"‚úì Concatenated with concat_ws()\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned string concatenation.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 15: String Functions - Substring and Length\n# Category: String Functions\n\n# Setup\ndata = [(\"Alice\",), (\"Bob\",), (\"Charlotte\",)]\ndf = spark.createDataFrame(data, [\"name\"])\n\n# Exercise (fill in the blanks)\n# Get the length of each name\nfrom pyspark.sql.functions import length, substring\n\nresult = df.withColumn(\"name_length\", ___(col(\"name\")))\nlengths = [row[\"name_length\"] for row in result.collect()]\nassert lengths == [5, 3, 9], f\"Expected [5, 3, 9], got {lengths}\"\nprint(\"‚úì Calculated string lengths\")\n\n# Get first 3 characters (substring is 1-indexed!)\nresult2 = df.withColumn(\"first_three\", ___(col(\"name\"), 1, 3))\nfirsts = [row[\"first_three\"] for row in result2.collect()]\nassert firsts == [\"Ali\", \"Bob\", \"Cha\"], f\"Expected ['Ali', 'Bob', 'Cha'], got {firsts}\"\nprint(\"‚úì Extracted first 3 characters\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned substring and length.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 16: String Functions - Trim and Pad\n# Category: String Functions\n\n# Setup\ndata = [(\"  Alice  \",), (\"Bob\",), (\" Charlie \",)]\ndf = spark.createDataFrame(data, [\"name\"])\n\n# Exercise (fill in the blanks)\n# Trim whitespace from both sides\nfrom pyspark.sql.functions import trim, ltrim, rtrim, lpad, rpad\n\nresult = df.withColumn(\"trimmed\", ___(col(\"name\")))\ntrimmed = [row[\"trimmed\"] for row in result.collect()]\nassert trimmed == [\"Alice\", \"Bob\", \"Charlie\"], f\"Expected trimmed names, got {trimmed}\"\nprint(\"‚úì Trimmed whitespace\")\n\n# Pad names to 10 characters with asterisks\nresult2 = df.withColumn(\"trimmed\", trim(col(\"name\")))\nresult2 = result2.withColumn(\"padded\", ___(col(\"trimmed\"), 10, \"*\"))\nassert result2.collect()[1][\"padded\"] == \"*******Bob\"\nprint(\"‚úì Left-padded with asterisks\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned trim and pad functions.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 17: Grouping and Aggregating\n# Category: Aggregations\n\n# Setup\ndata = [\n    (\"Sales\", \"Alice\", 5000),\n    (\"Sales\", \"Bob\", 4500),\n    (\"Engineering\", \"Charlie\", 6000),\n    (\"Engineering\", \"Diana\", 6500),\n    (\"Engineering\", \"Eve\", 5500)\n]\ndf = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"])\n\n# Exercise (fill in the blanks)\n# Group by department and calculate average salary\nfrom pyspark.sql.functions import avg, round\n\nresult = df.___(\"department\").agg(\n    round(___(\"salary\"), 2).alias(\"avg_salary\")\n)\n\n# Should have 2 departments\nassert result.count() == 2, f\"Expected 2 groups, got {result.count()}\"\nprint(\"‚úì Correct number of groups\")\n\n# Check Engineering average (6000 + 6500 + 5500) / 3 = 6000\neng_row = result.filter(col(\"department\") == \"Engineering\").collect()[0]\nassert eng_row[\"avg_salary\"] == 6000.0, f\"Expected 6000.0, got {eng_row['avg_salary']}\"\nprint(\"‚úì Engineering average salary is correct\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to group and aggregate.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 18: Multiple Aggregations\n# Category: Aggregations\n\n# Setup\ndata = [\n    (\"Sales\", 5000), (\"Sales\", 4500), (\"Sales\", 6000),\n    (\"Engineering\", 6000), (\"Engineering\", 6500)\n]\ndf = spark.createDataFrame(data, [\"department\", \"salary\"])\n\n# Exercise (fill in the blanks)\n# Calculate min, max, avg, and count per department\nfrom pyspark.sql.functions import min, max, avg, count\n\nresult = df.groupBy(\"department\").agg(\n    ___(\"salary\").alias(\"min_salary\"),\n    ___(\"salary\").alias(\"max_salary\"),\n    avg(\"salary\").alias(\"avg_salary\"),\n    ___(\"salary\").alias(\"emp_count\")\n)\n\nsales = result.filter(col(\"department\") == \"Sales\").collect()[0]\n\nassert sales[\"min_salary\"] == 4500, f\"Min should be 4500, got {sales['min_salary']}\"\nprint(\"‚úì Min salary correct\")\n\nassert sales[\"max_salary\"] == 6000, f\"Max should be 6000, got {sales['max_salary']}\"\nprint(\"‚úì Max salary correct\")\n\nassert sales[\"emp_count\"] == 3, f\"Count should be 3, got {sales['emp_count']}\"\nprint(\"‚úì Employee count correct\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned multiple aggregations.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 19: Aggregate Without Grouping\n# Category: Aggregations\n\n# Setup\ndata = [(100,), (200,), (300,), (400,), (500,)]\ndf = spark.createDataFrame(data, [\"value\"])\n\n# Exercise (fill in the blanks)\n# Calculate sum of all values without grouping\nfrom pyspark.sql.functions import sum as spark_sum, avg, count\n\nresult = df.___(spark_sum(\"value\").alias(\"total\"))\n\ntotal = result.collect()[0][\"total\"]\nassert total == 1500, f\"Expected 1500, got {total}\"\nprint(\"‚úì Sum calculated: 1500\")\n\n# Calculate multiple aggregates\nresult2 = df.agg(\n    spark_sum(\"value\").alias(\"total\"),\n    ___(\"value\").alias(\"average\"),\n    count(\"value\").alias(\"num_rows\")\n)\n\nrow = result2.collect()[0]\nassert row[\"average\"] == 300.0, f\"Expected 300.0, got {row['average']}\"\nassert row[\"num_rows\"] == 5, f\"Expected 5, got {row['num_rows']}\"\nprint(\"‚úì Multiple aggregates calculated\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned global aggregations.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 20: Inner Join\n# Category: Joins\n\n# Setup\nemployees = spark.createDataFrame([\n    (1, \"Alice\", 101),\n    (2, \"Bob\", 102),\n    (3, \"Charlie\", 101)\n], [\"emp_id\", \"name\", \"dept_id\"])\n\ndepartments = spark.createDataFrame([\n    (101, \"Engineering\"),\n    (102, \"Sales\"),\n    (103, \"Marketing\")\n], [\"dept_id\", \"dept_name\"])\n\n# Exercise (fill in the blanks)\n# Join employees with departments on dept_id\nresult = employees.___(departments, ___, \"inner\")\n\n# Should have 3 rows (all employees have matching departments)\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì Correct number of joined rows\")\n\n# Should have columns from both DataFrames\nassert \"name\" in result.columns, \"Missing 'name' column\"\nassert \"dept_name\" in result.columns, \"Missing 'dept_name' column\"\nprint(\"‚úì Columns from both DataFrames present\")\n\n# Alice should be in Engineering\nalice = result.filter(col(\"name\") == \"Alice\").collect()[0]\nassert alice[\"dept_name\"] == \"Engineering\", f\"Expected Engineering, got {alice['dept_name']}\"\nprint(\"‚úì Join matched correctly\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned inner joins.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 21: Left Outer Join\n# Category: Joins\n\n# Setup\nemployees = spark.createDataFrame([\n    (1, \"Alice\", 101),\n    (2, \"Bob\", 102),\n    (3, \"Charlie\", 999)  # No matching department!\n], [\"emp_id\", \"name\", \"dept_id\"])\n\ndepartments = spark.createDataFrame([\n    (101, \"Engineering\"),\n    (102, \"Sales\")\n], [\"dept_id\", \"dept_name\"])\n\n# Exercise (fill in the blanks)\n# Left join to keep all employees, even without matching dept\nresult = employees.join(departments, \"dept_id\", \"___\")\n\n# Should have 3 rows (all employees kept)\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì All employees kept\")\n\n# Charlie should have null department name\ncharlie = result.filter(col(\"name\") == \"Charlie\").collect()[0]\nassert charlie[\"dept_name\"] is None, f\"Expected None, got {charlie['dept_name']}\"\nprint(\"‚úì Charlie has no matching department (null)\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned left outer joins.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 22: Join on Multiple Columns\n# Category: Joins\n\n# Setup\norders = spark.createDataFrame([\n    (\"2024\", \"Q1\", \"Alice\", 100),\n    (\"2024\", \"Q2\", \"Alice\", 150),\n    (\"2024\", \"Q1\", \"Bob\", 200)\n], [\"year\", \"quarter\", \"rep\", \"amount\"])\n\ntargets = spark.createDataFrame([\n    (\"2024\", \"Q1\", 120),\n    (\"2024\", \"Q2\", 140)\n], [\"year\", \"quarter\", \"target\"])\n\n# Exercise (fill in the blanks)\n# Join on both year and quarter\nresult = orders.join(targets, [___, ___], \"inner\")\n\n# Should have 3 rows\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì Joined on multiple columns\")\n\n# Check that Alice Q1 has target 120\nalice_q1 = result.filter((col(\"rep\") == \"Alice\") & (col(\"quarter\") == \"Q1\")).collect()[0]\nassert alice_q1[\"target\"] == 120, f\"Expected target 120, got {alice_q1['target']}\"\nprint(\"‚úì Targets matched correctly\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned multi-column joins.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 23: Window Functions - Running Total\n# Category: Window Functions\n\n# Setup\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import sum as spark_sum, col\n\ndata = [\n    (\"2024-01-01\", 100),\n    (\"2024-01-02\", 150),\n    (\"2024-01-03\", 200),\n    (\"2024-01-04\", 175)\n]\ndf = spark.createDataFrame(data, [\"date\", \"sales\"])\n\n# Exercise (fill in the blanks)\n# Create a window that orders by date and includes all previous rows\nwindow_spec = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.___)\n\n# Add running total column\nresult = df.withColumn(\"running_total\", ___(\"sales\").over(window_spec))\n\n# Check the running totals\nrows = result.orderBy(\"date\").collect()\n\nassert rows[0][\"running_total\"] == 100, \"Day 1 should be 100\"\nprint(\"‚úì Day 1: 100\")\n\nassert rows[1][\"running_total\"] == 250, \"Day 2 should be 250 (100+150)\"\nprint(\"‚úì Day 2: 250\")\n\nassert rows[3][\"running_total\"] == 625, \"Day 4 should be 625\"\nprint(\"‚úì Day 4: 625 (cumulative)\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned window running totals.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 24: Window Functions - Row Number\n# Category: Window Functions\n\n# Setup\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, col\n\ndata = [\n    (\"Sales\", \"Alice\", 5000),\n    (\"Sales\", \"Bob\", 5500),\n    (\"Engineering\", \"Charlie\", 6000),\n    (\"Engineering\", \"Diana\", 6500),\n    (\"Engineering\", \"Eve\", 5500)\n]\ndf = spark.createDataFrame(data, [\"dept\", \"name\", \"salary\"])\n\n# Exercise (fill in the blanks)\n# Rank employees within each department by salary (highest first)\nwindow_spec = Window.partitionBy(\"___\").orderBy(col(\"salary\").desc())\n\nresult = df.withColumn(\"rank\", ___().___(window_spec))\n\n# Check rankings\neng = result.filter(col(\"dept\") == \"Engineering\").orderBy(\"rank\").collect()\nassert eng[0][\"name\"] == \"Diana\", f\"Diana should be #1 in Engineering, got {eng[0]['name']}\"\nassert eng[0][\"rank\"] == 1\nprint(\"‚úì Diana is #1 in Engineering ($6500)\")\n\nassert eng[1][\"name\"] == \"Charlie\", f\"Charlie should be #2, got {eng[1]['name']}\"\nprint(\"‚úì Charlie is #2 in Engineering ($6000)\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned row_number().\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 25: Window Functions - Lag and Lead\n# Category: Window Functions\n\n# Setup\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, lead, col\n\ndata = [\n    (\"2024-01-01\", 100),\n    (\"2024-01-02\", 150),\n    (\"2024-01-03\", 120),\n    (\"2024-01-04\", 200)\n]\ndf = spark.createDataFrame(data, [\"date\", \"price\"])\n\n# Exercise (fill in the blanks)\n# Get yesterday's price and calculate daily change\nwindow_spec = Window.orderBy(\"date\")\n\nresult = df.withColumn(\"prev_price\", ___(\"price\", 1).over(window_spec))\nresult = result.withColumn(\"change\", col(\"price\") - col(\"prev_price\"))\n\nrows = result.orderBy(\"date\").collect()\n\n# First row has no previous\nassert rows[0][\"prev_price\"] is None, \"First row should have no prev_price\"\nprint(\"‚úì First row has no previous\")\n\n# Second row: prev=100, change=50\nassert rows[1][\"prev_price\"] == 100, f\"Expected prev=100, got {rows[1]['prev_price']}\"\nassert rows[1][\"change\"] == 50, f\"Expected change=50, got {rows[1]['change']}\"\nprint(\"‚úì Day 2: prev=100, change=+50\")\n\n# Get tomorrow's price\nresult2 = df.withColumn(\"next_price\", ___(\"price\", 1).over(window_spec))\nrows2 = result2.orderBy(\"date\").collect()\nassert rows2[0][\"next_price\"] == 150, f\"Expected next=150, got {rows2[0]['next_price']}\"\nprint(\"‚úì Lead shows next day's price\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned lag and lead.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Null Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 26: Handling Nulls - Detection\n# Category: Null Handling\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", None), (\"Charlie\", 29), (None, 45)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Filter to rows where age is not null\nfrom pyspark.sql.functions import col, isnan, isnull\n\nresult = df.filter(col(\"age\").___())\n\nassert result.count() == 3, f\"Expected 3 rows with age, got {result.count()}\"\nprint(\"‚úì Filtered to non-null ages\")\n\n# Filter to rows where age IS null\nnulls = df.filter(col(\"age\").___())\nassert nulls.count() == 1, f\"Expected 1 null age, got {nulls.count()}\"\nprint(\"‚úì Found rows with null age\")\n\n# Check for null name\nnull_names = df.filter(col(\"name\").isNull())\nassert null_names.count() == 1\nprint(\"‚úì Found row with null name\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned null detection.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 27: Handling Nulls - Fill and Drop\n# Category: Null Handling\n\n# Setup\ndata = [(\"Alice\", 34), (\"Bob\", None), (None, 29), (\"Diana\", None)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Fill null ages with 0\nresult = df.___(0, subset=[\"age\"])\n\nages = [row[\"age\"] for row in result.collect()]\nassert None not in ages, \"Should have no null ages\"\nassert ages.count(0) == 2, \"Should have 2 zeros\"\nprint(\"‚úì Filled null ages with 0\")\n\n# Fill null names with \"Unknown\"\nresult2 = df.fillna(\"Unknown\", subset=[\"name\"])\nnames = [row[\"name\"] for row in result2.collect()]\nassert \"Unknown\" in names, \"Should have Unknown name\"\nprint(\"‚úì Filled null names\")\n\n# Drop rows with ANY null values\nresult3 = df.___()\nassert result3.count() == 1, f\"Expected 1 complete row, got {result3.count()}\"\nprint(\"‚úì Dropped rows with nulls\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to handle nulls.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 28: Union DataFrames\n# Category: Advanced\n\n# Setup\ndf1 = spark.createDataFrame([(\"Alice\", 34), (\"Bob\", 45)], [\"name\", \"age\"])\ndf2 = spark.createDataFrame([(\"Charlie\", 29), (\"Diana\", 52)], [\"name\", \"age\"])\n\n# Exercise (fill in the blanks)\n# Combine two DataFrames with the same schema\nresult = df1.___(df2)\n\nassert result.count() == 4, f\"Expected 4 rows, got {result.count()}\"\nprint(\"‚úì Combined DataFrames\")\n\nnames = [row[\"name\"] for row in result.collect()]\nassert \"Alice\" in names and \"Charlie\" in names, \"Should have names from both DFs\"\nprint(\"‚úì Contains data from both DataFrames\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to union DataFrames.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 29: Explode Arrays\n# Category: Advanced\n\n# Setup\nfrom pyspark.sql.functions import explode, split\n\ndata = [(\"Alice\", \"python,sql,spark\"), (\"Bob\", \"java,scala\")]\ndf = spark.createDataFrame(data, [\"name\", \"skills_str\"])\n\n# First split the string into an array\ndf = df.withColumn(\"skills\", split(col(\"skills_str\"), \",\"))\n\n# Exercise (fill in the blanks)\n# Explode the skills array into separate rows\nfrom pyspark.sql.functions import explode\n\nresult = df.select(\"name\", ___(col(\"skills\")).alias(\"skill\"))\n\nassert result.count() == 5, f\"Expected 5 rows, got {result.count()}\"\nprint(\"‚úì Exploded to 5 rows\")\n\nalice_skills = [row[\"skill\"] for row in result.filter(col(\"name\") == \"Alice\").collect()]\nassert len(alice_skills) == 3, f\"Alice should have 3 skills, got {len(alice_skills)}\"\nassert \"spark\" in alice_skills\nprint(\"‚úì Alice has 3 skills including spark\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned to explode arrays.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 30: Pivot Tables\n# Category: Advanced\n\n# Setup\ndata = [\n    (\"Alice\", \"Q1\", 100), (\"Alice\", \"Q2\", 150),\n    (\"Bob\", \"Q1\", 200), (\"Bob\", \"Q2\", 180)\n]\ndf = spark.createDataFrame(data, [\"name\", \"quarter\", \"sales\"])\n\n# Exercise (fill in the blanks)\n# Pivot to get quarters as columns\nfrom pyspark.sql.functions import sum as spark_sum\n\nresult = df.groupBy(\"name\").___(___).agg(spark_sum(\"sales\"))\n\n# Should have columns: name, Q1, Q2\nassert \"Q1\" in result.columns, \"Should have Q1 column\"\nassert \"Q2\" in result.columns, \"Should have Q2 column\"\nprint(\"‚úì Pivoted quarters to columns\")\n\nalice = result.filter(col(\"name\") == \"Alice\").collect()[0]\nassert alice[\"Q1\"] == 100, f\"Expected Q1=100, got {alice['Q1']}\"\nassert alice[\"Q2\"] == 150, f\"Expected Q2=150, got {alice['Q2']}\"\nprint(\"‚úì Values correctly placed in columns\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned pivot tables.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delta Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 101: Creating a Delta Table\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()  # Clean slate\n\ndata = [(\"Alice\", 34, \"Engineering\"), (\"Bob\", 45, \"Sales\"), (\"Charlie\", 29, \"Engineering\")]\ndf = spark.createDataFrame(data, [\"name\", \"age\", \"department\"])\n\n# Exercise (fill in the blanks)\n# Write the DataFrame as a Delta table\ndf.write.___(\"delta\").mode(\"overwrite\").save(\"/data/employees\")\n\n# Read it back\nresult = spark.read.format(\"___\").load(\"/data/employees\")\n\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì Delta table created and read successfully\")\n\n# Verify it's a Delta table\nassert DeltaTable.___(spark, \"/data/employees\"), \"Should be a Delta table\"\nprint(\"‚úì Confirmed as Delta table\")\n\nprint(\"\\\\nüéâ Koan complete! You've created your first Delta table.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 102: Time Travel - Version\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Create initial data (version 0)\ndata_v0 = [(\"Alice\", 100), (\"Bob\", 200)]\ndf_v0 = spark.createDataFrame(data_v0, [\"name\", \"balance\"])\ndf_v0.write.format(\"delta\").save(\"/data/accounts\")\n\n# Update data (version 1)\ndata_v1 = [(\"Alice\", 150), (\"Bob\", 250), (\"Charlie\", 300)]\ndf_v1 = spark.createDataFrame(data_v1, [\"name\", \"balance\"])\ndf_v1.write.format(\"delta\").mode(\"overwrite\").save(\"/data/accounts\")\n\n# Exercise (fill in the blanks)\n# Read the current version\ncurrent = spark.read.format(\"delta\").load(\"/data/accounts\")\nassert current.count() == 3, \"Current version should have 3 rows\"\nprint(f\"‚úì Current version has {current.count()} rows\")\n\n# Read version 0 using time travel\nhistorical = spark.read.format(\"delta\").option(\"___\", 0).load(\"/data/accounts\")\n\nassert historical.count() == 2, f\"Version 0 should have 2 rows, got {historical.count()}\"\nprint(f\"‚úì Version 0 has {historical.count()} rows\")\n\n# Check that Charlie wasn't in version 0\nnames_v0 = [row[\"name\"] for row in historical.collect()]\nassert \"Charlie\" not in names_v0, \"Charlie should not be in version 0\"\nprint(\"‚úì Charlie correctly absent from version 0\")\n\nprint(\"\\\\nüéâ Koan complete! You've mastered time travel queries.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 103: MERGE - Upsert Pattern\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Create target table\ntarget_data = [(\"Alice\", 100), (\"Bob\", 200)]\ntarget_df = spark.createDataFrame(target_data, [\"name\", \"balance\"])\ntarget_df.write.format(\"delta\").save(\"/data/accounts\")\n\n# Source data with updates and new records\nsource_data = [(\"Alice\", 150), (\"Charlie\", 300)]  # Alice updated, Charlie is new\nsource_df = spark.createDataFrame(source_data, [\"name\", \"balance\"])\n\n# Exercise (fill in the blanks)\n# Get the Delta table\ndt = DeltaTable.forPath(spark, \"/data/accounts\")\n\n# Merge source into target\n# When matched: update the balance\n# When not matched: insert the new record\ndt.___(\n    source_df,\n    \"target.name = source.name\"\n).whenMatched___().whenNotMatched___().execute()\n\n# Verify results\nresult = dt.toDF()\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì Correct row count after merge\")\n\n# Check Alice was updated\nalice = result.filter(col(\"name\") == \"Alice\").collect()[0]\nassert alice[\"balance\"] == 150, f\"Alice balance should be 150, got {alice['balance']}\"\nprint(\"‚úì Alice's balance updated to 150\")\n\n# Check Charlie was inserted\ncharlie = result.filter(col(\"name\") == \"Charlie\").collect()[0]\nassert charlie[\"balance\"] == 300, f\"Charlie balance should be 300\"\nprint(\"‚úì Charlie inserted with balance 300\")\n\nprint(\"\\\\nüéâ Koan complete! You've mastered the MERGE upsert pattern.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 104: MERGE - Selective Update\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Create target table with more columns\ntarget_data = [(\"Alice\", 100, \"2024-01-01\"), (\"Bob\", 200, \"2024-01-01\")]\ntarget_df = spark.createDataFrame(target_data, [\"name\", \"balance\", \"last_updated\"])\ntarget_df.write.format(\"delta\").save(\"/data/accounts\")\n\n# Source only has name and new balance\nsource_data = [(\"Alice\", 500)]\nsource_df = spark.createDataFrame(source_data, [\"name\", \"new_balance\"])\n\n# Exercise (fill in the blanks)\n# Get the Delta table\ndt = DeltaTable.forPath(spark, \"/data/accounts\")\n\n# Merge with specific column mapping\ndt.merge(\n    source_df,\n    \"target.name = source.name\"\n).whenMatchedUpdate(set={\n    \"___\": \"source.new_balance\",\n    \"last_updated\": \"'2024-06-01'\"\n}).execute()\n\n# Verify Alice's balance was updated\nresult = dt.toDF()\nalice = result.filter(col(\"name\") == \"Alice\").collect()[0]\n\nassert alice[\"balance\"] == 500, f\"Expected balance 500, got {alice['balance']}\"\nprint(\"‚úì Alice's balance updated to 500\")\n\n# Verify Bob was not changed\nbob = result.filter(col(\"name\") == \"Bob\").collect()[0]\nassert bob[\"balance\"] == 200, f\"Bob should still have 200, got {bob['balance']}\"\nprint(\"‚úì Bob's balance unchanged\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned selective MERGE updates.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 105: Table History\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Create and modify table multiple times\ndata = [(\"Alice\", 100)]\ndf = spark.createDataFrame(data, [\"name\", \"balance\"])\ndf.write.format(\"delta\").save(\"/data/accounts\")\n\n# Make some updates\ndata2 = [(\"Alice\", 100), (\"Bob\", 200)]\ndf2 = spark.createDataFrame(data2, [\"name\", \"balance\"])\ndf2.write.format(\"delta\").mode(\"overwrite\").save(\"/data/accounts\")\n\n# Exercise (fill in the blanks)\n# Get the Delta table\ndt = DeltaTable.forPath(spark, \"/data/accounts\")\n\n# Get the full history\nhistory_df = dt.___()\n\n# History should show our operations\nhistory_rows = history_df.collect()\nassert len(history_rows) >= 2, f\"Expected at least 2 history entries\"\nprint(f\"‚úì Found {len(history_rows)} history entries\")\n\n# Check version numbers exist\nversions = [row[\"version\"] for row in history_rows]\nassert 0 in versions, \"Should have version 0\"\nprint(\"‚úì Version 0 present in history\")\n\n# Check operations are recorded\noperations = [row[\"operation\"] for row in history_rows]\nprint(f\"‚úì Operations recorded: {operations}\")\n\nprint(\"\\\\nüéâ Koan complete! You can now audit Delta table changes.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 106: OPTIMIZE and Z-ORDER\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Create table with data\ndata = [(i, f\"user_{i}\", i % 10) for i in range(100)]\ndf = spark.createDataFrame(data, [\"id\", \"name\", \"category\"])\ndf.write.format(\"delta\").save(\"/data/users\")\n\n# Exercise (fill in the blanks)\n# Get the Delta table\ndt = DeltaTable.forPath(spark, \"/data/users\")\n\n# Optimize the table (compacts small files)\ndt.___()\nprint(\"‚úì Table optimized (files compacted)\")\n\n# Check history shows the optimize operation\nhistory = dt.history(1).collect()[0]\nassert history[\"operation\"] == \"OPTIMIZE\", f\"Expected OPTIMIZE, got {history['operation']}\"\nprint(\"‚úì OPTIMIZE recorded in history\")\n\n# Optimize with Z-ORDER for faster queries on specific columns\ndt.optimize().___(\"category\")\nprint(\"‚úì Z-ORDER optimization applied on 'category' column\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned table optimization.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 107: Delete with Condition\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\ndata = [(\"Alice\", 100, True), (\"Bob\", 200, False), (\"Charlie\", 150, True)]\ndf = spark.createDataFrame(data, [\"name\", \"balance\", \"is_active\"])\ndf.write.format(\"delta\").save(\"/data/accounts\")\n\n# Exercise (fill in the blanks)\n# Get the Delta table\ndt = DeltaTable.forPath(spark, \"/data/accounts\")\n\n# Delete all inactive accounts\ndt.___(condition=\"is_active == False\")\n\n# Verify Bob was deleted\nresult = dt.toDF()\nnames = [row[\"name\"] for row in result.collect()]\n\nassert \"Bob\" not in names, \"Bob should be deleted\"\nprint(\"‚úì Inactive account (Bob) deleted\")\n\nassert result.count() == 2, f\"Expected 2 rows, got {result.count()}\"\nprint(\"‚úì Only active accounts remain\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned Delta delete operations.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 108: Update with Condition\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\ndata = [(\"Alice\", 100, \"basic\"), (\"Bob\", 200, \"premium\"), (\"Charlie\", 50, \"basic\")]\ndf = spark.createDataFrame(data, [\"name\", \"balance\", \"tier\"])\ndf.write.format(\"delta\").save(\"/data/accounts\")\n\n# Exercise (fill in the blanks)\n# Get the Delta table\ndt = DeltaTable.forPath(spark, \"/data/accounts\")\n\n# Give all premium users a bonus: set their balance to balance + 100\ndt.___(\n    condition=\"tier == 'premium'\",\n    set_values={\"___\": 300}  # Bob's new balance\n)\n\n# Verify Bob got the bonus\nresult = dt.toDF()\nbob = result.filter(col(\"name\") == \"Bob\").collect()[0]\n\nassert bob[\"balance\"] == 300, f\"Bob should have 300, got {bob['balance']}\"\nprint(\"‚úì Premium user (Bob) balance updated\")\n\n# Verify others unchanged\nalice = result.filter(col(\"name\") == \"Alice\").collect()[0]\nassert alice[\"balance\"] == 100, f\"Alice should still have 100\"\nprint(\"‚úì Basic users unchanged\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned Delta update operations.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 109: Create Table with Builder\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Exercise (fill in the blanks)\n# Create a Delta table with explicit schema\nDeltaTable._____(spark) \\\\\n    .tableName(\"products\") \\\\\n    .addColumn(\"id\", \"INT\") \\\\\n    .addColumn(\"___\", \"STRING\") \\\\\n    .addColumn(\"price\", \"DOUBLE\") \\\\\n    .execute()\n\n# Verify table was created\nassert DeltaTable.isDeltaTable(spark, \"products\"), \"Table should exist\"\nprint(\"‚úì Table 'products' created\")\n\n# Get the table and verify schema\ndt = DeltaTable.forPath(spark, \"products\")\ndf = dt.toDF()\n\nassert \"name\" in df.columns, \"Should have 'name' column\"\nassert \"price\" in df.columns, \"Should have 'price' column\"\nprint(\"‚úì Table has correct columns\")\n\nassert df.count() == 0, \"New table should be empty\"\nprint(\"‚úì Table is empty (ready for data)\")\n\nprint(\"\\\\nüéâ Koan complete! You've learned the DeltaTable builder.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 110: VACUUM Old Files\n# Category: Delta Lake\n\n# Setup\n_reset_delta_tables()\n\n# Create table and make several updates\nfor i in range(3):\n    data = [(j, f\"version_{i}\") for j in range(10)]\n    df = spark.createDataFrame(data, [\"id\", \"data\"])\n    df.write.format(\"delta\").mode(\"overwrite\").save(\"/data/versions\")\n\n# Exercise (fill in the blanks)\n# Get the Delta table  \ndt = DeltaTable.forPath(spark, \"/data/versions\")\n\n# Check we have multiple versions\nhistory = dt.history()\nversion_count = history.count()\nprint(f\"Table has {version_count} versions\")\n\n# Vacuum to remove old files\n# Default retention is 168 hours (7 days)\nresult = dt.___(retention_hours=168)\n\nprint(\"‚úì Vacuum completed\")\n\n# Note: After vacuum, time travel to old versions may not work!\n# This is a trade-off between storage and history access\nprint(\"\\\\n‚ö†Ô∏è  Warning: VACUUM removes old version files!\")\nprint(\"   Time travel to vacuumed versions will fail.\")\n\nprint(\"\\\\nüéâ Koan complete! You understand VACUUM and retention.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unity Catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 201: Creating Catalogs and Schemas\n# Category: Unity Catalog\n\n# Setup\n# Unity Catalog uses a three-level namespace: catalog.schema.table\n# First, let's see what's available\nspark.sql(\"SHOW CATALOGS\").show()\n\n# Exercise (fill in the blanks)\n# Create a new catalog\nspark.sql(\"___ CATALOG my_catalog\")\nprint(\"‚úì Catalog 'my_catalog' created\")\n\n# Create a schema (database) within the catalog\nspark.sql(\"CREATE ___ my_catalog.my_schema\")\nprint(\"‚úì Schema 'my_catalog.my_schema' created\")\n\n# Verify the schema was created\nschemas = spark.sql(\"SHOW SCHEMAS IN ___\").collect()\nschema_names = [row.databaseName for row in schemas]\nassert \"my_schema\" in schema_names, f\"Schema not found. Available: {schema_names}\"\nprint(\"‚úì Schema verified in catalog\")\n\n# Set the current schema for subsequent operations\nspark.sql(\"___ my_catalog.my_schema\")\nprint(\"‚úì Current schema set to my_catalog.my_schema\")\n\nprint(\"\\\\nüéâ Koan complete! You've created a Unity Catalog namespace.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 202: Creating Managed Tables\n# Category: Unity Catalog\n\n# Setup\n# Set up a catalog and schema\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.demo_schema\")\nspark.sql(\"USE demo_catalog.demo_schema\")\n\n# Create sample data\ndata = [(\"Alice\", 85), (\"Bob\", 92), (\"Charlie\", 78)]\ndf = spark.createDataFrame(data, [\"name\", \"score\"])\n\n# Exercise (fill in the blanks)\n# Create a managed table from the DataFrame\n# Managed tables have their data managed by Unity Catalog\ndf.write.saveAsTable(\"___\")\nprint(\"‚úì Managed table 'students' created\")\n\n# Read back the table using the three-level namespace\nresult = spark.table(\"demo_catalog.demo_schema.___\")\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì Table data verified\")\n\n# Show table properties\nspark.sql(\"___ TABLE demo_catalog.demo_schema.students\").show()\nprint(\"‚úì Table description displayed\")\n\nprint(\"\\\\nüéâ Koan complete! You've created a managed table in Unity Catalog.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 203: Creating External Tables\n# Category: Unity Catalog\n\n# Setup\n# Set up namespace\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.demo_schema\")\nspark.sql(\"USE demo_catalog.demo_schema\")\n\n# Create and save data to a specific location\ndata = [(\"Product A\", 100), (\"Product B\", 250), (\"Product C\", 175)]\ndf = spark.createDataFrame(data, [\"product\", \"price\"])\ndf.write.format(\"delta\").save(\"/data/products_external\")\n\n# Exercise (fill in the blanks)\n# Create an external table pointing to existing data\n# External tables don't manage the underlying data files\nspark.sql(\"\"\"\n  CREATE TABLE products_ext\n  USING ___\n  ___ '/data/products_external'\n\"\"\")\nprint(\"‚úì External table 'products_ext' created\")\n\n# Query the external table\nresult = spark.sql(\"SELECT * FROM products_ext\")\nassert result.count() == 3, f\"Expected 3 rows, got {result.count()}\"\nprint(\"‚úì External table data accessible\")\n\n# Check if table is external\ntable_info = spark.sql(\"DESCRIBE ___ products_ext\").collect()\nproperties = {row.col_name: row.data_type for row in table_info}\nprint(\"‚úì Table metadata retrieved\")\n\nprint(\"\\\\nüéâ Koan complete! You understand managed vs external tables.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 204: Granting Permissions\n# Category: Unity Catalog\n\n# Setup\n# Set up namespace and table\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.demo_schema\")\nspark.sql(\"USE demo_catalog.demo_schema\")\n\ndata = [(\"Alice\", 1000), (\"Bob\", 2000)]\ndf = spark.createDataFrame(data, [\"name\", \"salary\"])\ndf.write.mode(\"overwrite\").saveAsTable(\"employees\")\n\n# Exercise (fill in the blanks)\n# Grant SELECT permission to a user\nspark.sql(\"___ SELECT ON TABLE employees TO \\"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 205: Revoking Permissions\n# Category: Unity Catalog\n\n# Setup\n# Set up namespace and table\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.demo_schema\")\nspark.sql(\"USE demo_catalog.demo_schema\")\n\ndata = [(\"Customer A\", \"Premium\"), (\"Customer B\", \"Basic\")]\ndf = spark.createDataFrame(data, [\"customer\", \"tier\"])\ndf.write.mode(\"overwrite\").saveAsTable(\"customers\")\n\n# Grant some permissions first\nspark.sql(\"GRANT SELECT ON TABLE customers TO \\\n\n# Exercise (fill in the blanks)\n# Revoke SELECT permission from a user\nspark.sql(\"___ SELECT ON TABLE customers FROM \\"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 206: Querying Information Schema\n# Category: Unity Catalog\n\n# Setup\n# Set up catalog with multiple objects\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.analytics\")\nspark.sql(\"USE demo_catalog.analytics\")\n\n# Create several tables\nspark.createDataFrame([(\"a\", 1)], [\"col1\", \"col2\"]).write.mode(\"overwrite\").saveAsTable(\"table1\")\nspark.createDataFrame([(\"b\", 2)], [\"col1\", \"col2\"]).write.mode(\"overwrite\").saveAsTable(\"table2\")\n\n# Exercise (fill in the blanks)\n# Query all tables in the current schema using information_schema\ntables_df = spark.sql(\"\"\"\n  SELECT table_name\n  FROM ___.tables\n  WHERE table_schema = 'analytics'\n\"\"\")\nprint(\"‚úì Queried information_schema.tables\")\n\ntable_count = tables_df.count()\nassert table_count >= 2, f\"Expected at least 2 tables, got {table_count}\"\nprint(f\"‚úì Found {table_count} tables in schema\")\n\n# Query column information\ncolumns_df = spark.sql(\"\"\"\n  SELECT table_name, column_name, data_type\n  FROM information_schema.___\n  WHERE table_schema = 'analytics' AND table_name = 'table1'\n\"\"\")\nprint(\"‚úì Queried column metadata\")\n\ncol_count = columns_df.count()\nassert col_count == 2, f\"Expected 2 columns, got {col_count}\"\nprint(\"‚úì Column information retrieved\")\n\n# Query all schemas in the catalog\nschemas_df = spark.sql(\"\"\"\n  SELECT schema_name\n  FROM information_schema.___\n  WHERE catalog_name = 'demo_catalog'\n\"\"\")\nprint(\"‚úì Queried schema information\")\n\nprint(\"\\\\nüéâ Koan complete! You can now discover Unity Catalog metadata.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 207: Table Properties and Comments\n# Category: Unity Catalog\n\n# Setup\n# Set up namespace\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.metadata_demo\")\nspark.sql(\"USE demo_catalog.metadata_demo\")\n\n# Exercise (fill in the blanks)\n# Create a table with a comment\nspark.sql(\"\"\"\n  CREATE TABLE products (\n    product_id INT,\n    product_name STRING,\n    price DOUBLE\n  )\n  ___ 'Product catalog table containing all available products'\n\"\"\")\nprint(\"‚úì Table created with comment\")\n\n# Add a property to the table\nspark.sql(\"\"\"\n  ALTER TABLE products\n  SET ___ ('owner' = 'data_team', 'pii' = 'false')\n\"\"\")\nprint(\"‚úì Table properties set\")\n\n# Add a comment to a specific column\nspark.sql(\"\"\"\n  ALTER TABLE products\n  ALTER COLUMN price ___ 'Price in USD'\n\"\"\")\nprint(\"‚úì Column comment added\")\n\n# View table properties\nspark.sql(\"DESCRIBE EXTENDED products\").show()\nprint(\"‚úì Table metadata displayed\")\n\nprint(\"\\\\nüéâ Koan complete! You can now document tables with metadata.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 208: Creating Views in Unity Catalog\n# Category: Unity Catalog\n\n# Setup\n# Set up namespace and base table\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.views_demo\")\nspark.sql(\"USE demo_catalog.views_demo\")\n\ndata = [\n  (\"Alice\", \"Engineering\", 85000),\n  (\"Bob\", \"Sales\", 75000),\n  (\"Charlie\", \"Engineering\", 95000),\n  (\"Diana\", \"Sales\", 80000)\n]\ndf = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])\ndf.write.mode(\"overwrite\").saveAsTable(\"employees\")\n\n# Exercise (fill in the blanks)\n# Create a view that filters high earners\nspark.sql(\"\"\"\n  CREATE ___ high_earners AS\n  SELECT name, department, salary\n  FROM employees\n  WHERE salary > 80000\n\"\"\")\nprint(\"‚úì View 'high_earners' created\")\n\n# Query the view\nresult = spark.sql(\"SELECT * FROM ___\")\nassert result.count() == 2, f\"Expected 2 high earners, got {result.count()}\"\nprint(\"‚úì View query successful\")\n\n# Create a temporary view (session-scoped)\nspark.sql(\"\"\"\n  CREATE ___ VIEW sales_team AS\n  SELECT name, salary\n  FROM employees\n  WHERE department = 'Sales'\n\"\"\")\nprint(\"‚úì Temporary view created\")\n\n# Verify temporary view works\ntemp_result = spark.sql(\"SELECT * FROM sales_team\")\nassert temp_result.count() == 2, f\"Expected 2 sales members\"\nprint(\"‚úì Temporary view query successful\")\n\nprint(\"\\\\nüéâ Koan complete! You can now use views to simplify queries.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 209: Inspecting Table ACLs\n# Category: Unity Catalog\n\n# Setup\n# Set up namespace and table\nspark.sql(\"CREATE CATALOG IF NOT EXISTS demo_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_catalog.acl_demo\")\nspark.sql(\"USE demo_catalog.acl_demo\")\n\ndata = [(\"record1\", 100), (\"record2\", 200)]\ndf = spark.createDataFrame(data, [\"id\", \"value\"])\ndf.write.mode(\"overwrite\").saveAsTable(\"secure_data\")\n\n# Grant some permissions for testing\nspark.sql(\"GRANT SELECT ON TABLE secure_data TO \\\n\n# Exercise (fill in the blanks)\n# Show grants on the table\ngrants_df = spark.sql(\"___ GRANTS ON TABLE secure_data\")\ngrants_df.show()\nprint(\"‚úì Grants displayed\")\n\ngrant_count = grants_df.count()\nassert grant_count >= 2, f\"Expected at least 2 grants, got {grant_count}\"\nprint(f\"‚úì Found {grant_count} grant entries\")\n\n# Check grants for a specific principal\nuser_grants = spark.sql(\"\"\"\n  SHOW ___ ON TABLE secure_data\n\"\"\").filter(\"principal = 'analyst@example.com'\")\nprint(\"‚úì Filtered grants for specific user\")\n\n# Show all grants on the schema\nschema_grants = spark.sql(\"SHOW GRANTS ON ___ acl_demo\")\nprint(\"‚úì Schema grants displayed\")\n\nprint(\"\\\\nüéâ Koan complete! You can now audit Unity Catalog permissions.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 210: Three-Level Namespace Usage\n# Category: Unity Catalog\n\n# Setup\n# Create multiple catalogs and schemas\nspark.sql(\"CREATE CATALOG IF NOT EXISTS prod_catalog\")\nspark.sql(\"CREATE CATALOG IF NOT EXISTS dev_catalog\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS prod_catalog.sales\")\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS dev_catalog.sales\")\n\n# Create tables in different catalogs\nspark.sql(\"USE prod_catalog.sales\")\nspark.createDataFrame([(\"Product A\", 1000)], [\"name\", \"revenue\"]).write.mode(\"overwrite\").saveAsTable(\"revenue\")\n\nspark.sql(\"USE dev_catalog.sales\")\nspark.createDataFrame([(\"Product A\", 500)], [\"name\", \"revenue\"]).write.mode(\"overwrite\").saveAsTable(\"revenue\")\n\n# Exercise (fill in the blanks)\n# Query from production using full three-level name\nprod_df = spark.sql(\"SELECT * FROM ___.sales.revenue\")\nprod_revenue = prod_df.collect()[0][\"revenue\"]\nassert prod_revenue == 1000, f\"Expected prod revenue 1000, got {prod_revenue}\"\nprint(\"‚úì Queried production table using full path\")\n\n# Query from development\ndev_df = spark.sql(\"SELECT * FROM dev_catalog.___.___\")\ndev_revenue = dev_df.collect()[0][\"revenue\"]\nassert dev_revenue == 500, f\"Expected dev revenue 500, got {dev_revenue}\"\nprint(\"‚úì Queried development table using full path\")\n\n# Join tables across catalogs\nresult = spark.sql(\"\"\"\n  SELECT\n    p.name,\n    p.revenue as prod_revenue,\n    d.revenue as dev_revenue\n  FROM prod_catalog.sales.revenue p\n  JOIN ___.sales.revenue d ON p.name = d.name\n\"\"\")\nprint(\"‚úì Joined tables across catalogs\")\n\n# Get current catalog and schema\ncurrent_catalog = spark.sql(\"SELECT ___()\").collect()[0][0]\nprint(f\"‚úì Current catalog: {current_catalog}\")\n\ncurrent_schema = spark.sql(\"SELECT current_database()\").collect()[0][0]\nprint(f\"‚úì Current schema: {current_schema}\")\n\nprint(\"\\\\nüéâ Koan complete! You've mastered Unity Catalog namespacing.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pandas API on Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 301: Converting to Pandas API on Spark\n# Category: Pandas API on Spark\n\n# Setup\n# Create a PySpark DataFrame\ndata = [(\"Alice\", 25, \"NYC\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 35, \"Chicago\")]\nspark_df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\nprint(\"PySpark DataFrame created\")\n\n# Exercise (fill in the blanks)\n# Convert PySpark DataFrame to Pandas-on-Spark DataFrame\nimport pyspark.pandas as ps\n\n# Method 1: Convert from PySpark DataFrame\npsdf = spark_df.___()\nprint(\"‚úì Converted to Pandas-on-Spark DataFrame\")\n\n# Check the type\nprint(f\"Type: {type(psdf)}\")\nassert \"pandas\" in str(type(psdf)).lower(), \"Should be a pandas-on-spark DataFrame\"\nprint(\"‚úì Type verified\")\n\n# Pandas-on-Spark supports familiar pandas operations\nprint(\"\\\\nDataFrame head:\")\nprint(psdf.___(2))  # Show first 2 rows\nprint(\"‚úì head() works like pandas\")\n\n# Method 2: Create directly from data\npsdf2 = ps.___({\"name\": [\"Dave\", \"Eve\"], \"age\": [28, 32]})\nprint(\"‚úì Created Pandas-on-Spark DataFrame from dict\")\n\nprint(\"\\\\nüéâ Koan complete! You can now use pandas syntax with Spark.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 302: Indexing with .loc and .iloc\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a Pandas-on-Spark DataFrame\npsdf = ps.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"age\": [25, 30, 35, 28],\n    \"city\": [\"NYC\", \"LA\", \"Chicago\", \"Boston\"]\n})\nprint(\"DataFrame created:\")\nprint(psdf)\n\n# Exercise (fill in the blanks)\n# Use .loc to select by label/condition\n# Select rows where age > 28\nresult1 = psdf.___[psdf[\"age\"] > 28]\nprint(\"\\\\nRows with age > 28:\")\nprint(result1)\nassert len(result1) == 2, f\"Expected 2 rows, got {len(result1)}\"\nprint(\"‚úì .loc with condition works\")\n\n# Use .loc to select specific columns\nresult2 = psdf.loc[:, [\"name\", \"___\"]]\nprint(\"\\\\nSelected name and city columns:\")\nprint(result2)\nassert \"age\" not in result2.columns.tolist(), \"age should not be included\"\nprint(\"‚úì .loc column selection works\")\n\n# Use .iloc for integer-based indexing\n# Get the first 2 rows\nresult3 = psdf.___[:2]\nprint(\"\\\\nFirst 2 rows (using iloc):\")\nprint(result3)\nassert len(result3) == 2, f\"Expected 2 rows, got {len(result3)}\"\nprint(\"‚úì .iloc position indexing works\")\n\n# Get specific row and column by position\nvalue = psdf.iloc[1, 0]  # Second row, first column\nprint(f\"\\\\nValue at [1, 0]: {value}\")\nassert value == \"Bob\", f\"Expected 'Bob', got {value}\"\nprint(\"‚úì .iloc single value access works\")\n\nprint(\"\\\\nüéâ Koan complete! You can now use pandas-style indexing.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 303: DataFrame Operations\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a Pandas-on-Spark DataFrame\npsdf = ps.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n    \"price\": [100, 250, 175, 300, 125],\n    \"quantity\": [10, 5, 8, 3, 12]\n})\n\n# Exercise (fill in the blanks)\n# Show first 3 rows\nprint(\"First 3 rows:\")\nprint(psdf.___(3))\nprint(\"‚úì head() works\")\n\n# Show last 2 rows\nprint(\"\\\\nLast 2 rows:\")\nprint(psdf.___(2))\nprint(\"‚úì tail() works\")\n\n# Get summary statistics\nprint(\"\\\\nSummary statistics:\")\nstats = psdf.___()\nprint(stats)\nprint(\"‚úì describe() provides statistics\")\n\n# Get shape (rows, columns)\nrows, cols = psdf.___\nprint(f\"\\\\nShape: {rows} rows, {cols} columns\")\nassert rows == 5, f\"Expected 5 rows\"\nassert cols == 3, f\"Expected 3 columns\"\nprint(\"‚úì shape attribute works\")\n\n# Get column data types\nprint(\"\\\\nData types:\")\nprint(psdf.___)\nprint(\"‚úì dtypes shows column types\")\n\n# Get column names\ncolumns = psdf.___.tolist()\nprint(f\"\\\\nColumns: {columns}\")\nassert \"price\" in columns, \"Should have 'price' column\"\nprint(\"‚úì columns attribute works\")\n\nprint(\"\\\\nüéâ Koan complete! You know pandas DataFrame operations.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 304: Series Operations\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a DataFrame\npsdf = ps.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"score\": [85, 92, 78, 88]\n})\n\n# Exercise (fill in the blanks)\n# Extract a Series (single column)\nscores = psdf[\"___\"]\nprint(\"Scores Series:\")\nprint(scores)\nprint(f\"Type: {type(scores)}\")\nprint(\"‚úì Extracted Series from DataFrame\")\n\n# Get Series statistics\nmean_score = scores.___()\nprint(f\"\\\\nMean score: {mean_score}\")\nassert mean_score > 80, f\"Mean should be > 80\"\nprint(\"‚úì mean() calculated\")\n\nmax_score = scores.___()\nprint(f\"Max score: {max_score}\")\nassert max_score == 92, f\"Max should be 92\"\nprint(\"‚úì max() found\")\n\n# Apply operations to Series\n# Add 5 bonus points to all scores\nadjusted_scores = scores + ___\nprint(\"\\\\nAdjusted scores (bonus +5):\")\nprint(adjusted_scores)\nprint(\"‚úì Arithmetic operations work on Series\")\n\n# Filter Series\nhigh_scores = scores[scores >= ___]\nprint(\"\\\\nScores >= 85:\")\nprint(high_scores)\nassert len(high_scores) == 3, f\"Expected 3 high scores\"\nprint(\"‚úì Boolean filtering works on Series\")\n\n# Get unique values\nnames_series = psdf[\"name\"]\nunique_names = names_series.___()\nprint(f\"\\\\nUnique names count: {len(unique_names)}\")\nprint(\"‚úì unique() works on Series\")\n\nprint(\"\\\\nüéâ Koan complete! You can now work with Series.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 305: Index Operations\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a DataFrame with default integer index\npsdf = ps.DataFrame({\n    \"employee_id\": [101, 102, 103, 104],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"salary\": [75000, 82000, 68000, 91000]\n})\n\n# Exercise (fill in the blanks)\n# Set a column as the index\npsdf_indexed = psdf.set_index(\"___\")\nprint(\"DataFrame with employee_id as index:\")\nprint(psdf_indexed)\nprint(\"‚úì set_index() applied\")\n\n# Access the index\nindex_values = psdf_indexed.___.tolist()\nprint(f\"\\\\nIndex values: {index_values}\")\nassert 101 in index_values, \"101 should be in index\"\nprint(\"‚úì index attribute accessed\")\n\n# Reset the index back to default\npsdf_reset = psdf_indexed.___()\nprint(\"\\\\nDataFrame with reset index:\")\nprint(psdf_reset)\nprint(\"‚úì reset_index() applied\")\n\n# Sort by index\npsdf_indexed_desc = psdf_indexed.sort_index(ascending=___)\nprint(\"\\\\nSorted by index (descending):\")\nprint(psdf_indexed_desc)\nprint(\"‚úì sort_index() works\")\n\n# Set index name\npsdf_indexed.index.___ = \"emp_id\"\nprint(f\"\\\\nIndex name: {psdf_indexed.index.name}\")\nassert psdf_indexed.index.name == \"emp_id\", \"Index name should be 'emp_id'\"\nprint(\"‚úì Index name set\")\n\nprint(\"\\\\nüéâ Koan complete! You can now manage DataFrame indexes.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 306: GroupBy with Pandas Syntax\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a sales DataFrame\npsdf = ps.DataFrame({\n    \"region\": [\"East\", \"East\", \"West\", \"West\", \"East\", \"West\"],\n    \"product\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n    \"sales\": [100, 150, 200, 180, 120, 160]\n})\n\n# Exercise (fill in the blanks)\n# Group by region and calculate mean sales\nregion_avg = psdf.groupby(\"___\")[\"sales\"].mean()\nprint(\"Average sales by region:\")\nprint(region_avg)\nprint(\"‚úì groupby() with aggregation works\")\n\n# Group by multiple columns\nmulti_group = psdf.groupby([\"region\", \"___\"])[\"sales\"].sum()\nprint(\"\\\\nTotal sales by region and product:\")\nprint(multi_group)\nprint(\"‚úì Multi-column groupby works\")\n\n# Use agg() for multiple aggregations\nresult = psdf.groupby(\"region\").___({\n    \"sales\": [\"sum\", \"mean\", \"___\"]\n})\nprint(\"\\\\nMultiple aggregations:\")\nprint(result)\nprint(\"‚úì agg() with multiple functions works\")\n\n# Group and get size (count of rows per group)\ngroup_sizes = psdf.groupby(\"region\").___()\nprint(\"\\\\nGroup sizes:\")\nprint(group_sizes)\nassert group_sizes[\"East\"] == 3, f\"East should have 3 rows\"\nprint(\"‚úì size() counts rows per group\")\n\n# Apply custom aggregation\nmax_sales_by_region = psdf.groupby(\"region\")[\"sales\"].___()\nprint(\"\\\\nMax sales by region:\")\nprint(max_sales_by_region)\nprint(\"‚úì max() aggregation works\")\n\nprint(\"\\\\nüéâ Koan complete! You can use pandas-style groupby.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 307: Merge and Join with Pandas Syntax\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create two DataFrames\nemployees = ps.DataFrame({\n    \"emp_id\": [1, 2, 3, 4],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"dept_id\": [10, 20, 10, 30]\n})\n\ndepartments = ps.DataFrame({\n    \"dept_id\": [10, 20, 30],\n    \"dept_name\": [\"Engineering\", \"Sales\", \"HR\"]\n})\n\n# Exercise (fill in the blanks)\n# Merge on a common column\nresult = employees.merge(departments, on=\"___\")\nprint(\"Merged DataFrame:\")\nprint(result)\nassert len(result) == 4, f\"Expected 4 rows after merge\"\nprint(\"‚úì merge() on common column works\")\n\n# Left join (keep all employees even if no dept match)\n# First add an employee with no department\nemployees_extended = ps.DataFrame({\n    \"emp_id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n    \"dept_id\": [10, 20, 10, 30, 99]  # 99 doesn't exist in departments\n})\n\nleft_result = employees_extended.merge(departments, on=\"dept_id\", how=\"___\")\nprint(\"\\\\nLeft join result:\")\nprint(left_result)\nassert len(left_result) == 5, f\"Expected 5 rows in left join\"\nprint(\"‚úì Left join preserves all left rows\")\n\n# Merge with different column names\nproducts = ps.DataFrame({\n    \"product_id\": [1, 2, 3],\n    \"product_name\": [\"Widget\", \"Gadget\", \"Doohickey\"]\n})\n\nsales = ps.DataFrame({\n    \"sale_id\": [101, 102, 103],\n    \"prod_id\": [1, 2, 1],  # Note: different column name\n    \"amount\": [100, 200, 150]\n})\n\n# Merge using left_on and right_on\nmerged_sales = sales.merge(\n    products,\n    left_on=\"___\",\n    right_on=\"product_id\"\n)\nprint(\"\\\\nMerged sales with products:\")\nprint(merged_sales)\nprint(\"‚úì merge() with different column names works\")\n\nprint(\"\\\\nüéâ Koan complete! You can now merge DataFrames like pandas.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 308: String Methods (.str accessor)\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a DataFrame with string data\npsdf = ps.DataFrame({\n    \"name\": [\"  Alice  \", \"BOB\", \"charlie\", \"DIANA\"],\n    \"email\": [\"alice@example.com\", \"bob@test.com\", \"charlie@demo.org\", \"diana@example.com\"]\n})\n\n# Exercise (fill in the blanks)\n# Convert to lowercase\nnames_lower = psdf[\"name\"].str.___()\nprint(\"Lowercase names:\")\nprint(names_lower)\nprint(\"‚úì .str.lower() works\")\n\n# Convert to uppercase\nnames_upper = psdf[\"name\"].str.___()\nprint(\"\\\\nUppercase names:\")\nprint(names_upper)\nprint(\"‚úì .str.upper() works\")\n\n# Strip whitespace\nnames_clean = psdf[\"name\"].str.___()\nprint(\"\\\\nCleaned names (stripped):\")\nprint(names_clean)\nassert \"  \" not in names_clean.tolist()[0], \"Whitespace should be removed\"\nprint(\"‚úì .str.strip() removes whitespace\")\n\n# Check if string contains a pattern\nhas_example = psdf[\"email\"].str.___(\\\"example\\\")\nprint(\"\\\\nEmails containing 'example':\")\nprint(has_example)\nexample_count = has_example.sum()\nassert example_count == 2, f\"Expected 2 emails with 'example'\"\nprint(\"‚úì .str.contains() works\")\n\n# Extract domain from email (split on @)\ndomains = psdf[\"email\"].str.split(\"@\").str.___(-1)\nprint(\"\\\\nEmail domains:\")\nprint(domains)\nprint(\"‚úì .str.split() and indexing works\")\n\n# String length\nname_lengths = psdf[\"name\"].str.___()\nprint(\"\\\\nName lengths:\")\nprint(name_lengths)\nprint(\"‚úì .str.len() works\")\n\nprint(\"\\\\nüéâ Koan complete! You can now manipulate strings with .str accessor.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 309: Type Conversion and Casting\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a DataFrame with mixed types\npsdf = ps.DataFrame({\n    \"id\": [\"1\", \"2\", \"3\", \"4\"],  # String that should be int\n    \"price\": [\"10.5\", \"20.3\", \"15.7\", \"30.2\"],  # String that should be float\n    \"quantity\": [10, 20, 15, 30]  # Already int\n})\n\nprint(\"Original DataFrame:\")\nprint(psdf)\nprint(\"\\\\nOriginal dtypes:\")\nprint(psdf.dtypes)\n\n# Exercise (fill in the blanks)\n# Convert string to integer\npsdf[\"id\"] = psdf[\"id\"].astype(___)\nprint(\"\\\\nAfter converting id to int:\")\nprint(psdf.dtypes[\"id\"])\nassert psdf.dtypes[\"id\"] == \"int64\" or \"int\" in str(psdf.dtypes[\"id\"]), \"id should be integer\"\nprint(\"‚úì Converted to int\")\n\n# Convert string to float\npsdf[\"price\"] = psdf[\"price\"].astype(___)\nprint(\"\\\\nAfter converting price to float:\")\nprint(psdf.dtypes[\"price\"])\nassert \"float\" in str(psdf.dtypes[\"price\"]), \"price should be float\"\nprint(\"‚úì Converted to float\")\n\n# Convert to string\npsdf[\"quantity_str\"] = psdf[\"quantity\"].astype(___)\nprint(\"\\\\nCreated quantity_str:\")\nprint(psdf[[\"quantity\", \"quantity_str\"]])\nassert psdf.dtypes[\"quantity_str\"] == \"object\" or \"str\" in str(psdf.dtypes[\"quantity_str\"]), \"Should be string\"\nprint(\"‚úì Converted to string\")\n\n# Check current dtypes\nprint(\"\\\\nFinal dtypes:\")\nprint(psdf.___)\nprint(\"‚úì All conversions complete\")\n\n# Perform calculations with converted types\ntotal_value = (psdf[\"price\"] * psdf[\"quantity\"]).sum()\nprint(f\"\\\\nTotal value: \\${total_value:.2f}\")\nassert total_value > 0, \"Total value should be calculated\"\nprint(\"‚úì Calculations work with converted types\")\n\nprint(\"\\\\nüéâ Koan complete! You can now convert data types.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Koan 310: Converting to Spark and Pandas\n# Category: Pandas API on Spark\n\n# Setup\nimport pyspark.pandas as ps\n\n# Create a Pandas-on-Spark DataFrame\npsdf = ps.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"score\": [85, 92, 78],\n    \"grade\": [\"B\", \"A\", \"C\"]\n})\n\nprint(\"Pandas-on-Spark DataFrame:\")\nprint(psdf)\n\n# Exercise (fill in the blanks)\n# Convert to PySpark DataFrame\nspark_df = psdf.___()\nprint(\"\\\\nConverted to PySpark DataFrame:\")\nprint(f\"Type: {type(spark_df)}\")\nassert \"pyspark.sql\" in str(type(spark_df)), \"Should be PySpark DataFrame\"\nprint(\"‚úì Converted to PySpark DataFrame\")\n\n# Now we can use PySpark operations\nfrom pyspark.sql.functions import col\nspark_filtered = spark_df.filter(col(\"score\") > 80)\nprint(f\"Filtered count: {spark_filtered.count()}\")\nprint(\"‚úì PySpark operations work\")\n\n# Convert PySpark back to Pandas-on-Spark\npsdf2 = spark_df.___()\nprint(\"\\\\nConverted back to Pandas-on-Spark:\")\nprint(f\"Type: {type(psdf2)}\")\nprint(\"‚úì Converted back to Pandas-on-Spark\")\n\n# Convert to regular Pandas (collects data to driver)\n# WARNING: Only do this for small datasets!\npandas_df = psdf.___()\nprint(\"\\\\nConverted to regular Pandas:\")\nprint(f\"Type: {type(pandas_df)}\")\nassert \"pandas\" in str(type(pandas_df)).lower(), \"Should be pandas DataFrame\"\nprint(\"‚úì Converted to regular Pandas\")\n\n# Use regular pandas operations\nprint(\"\\\\nPandas describe:\")\nprint(pandas_df.describe())\nprint(\"‚úì Regular pandas operations work\")\n\nprint(\"\\\\nüéâ Koan complete! You can convert between Spark and Pandas.\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}