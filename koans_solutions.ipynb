{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Koans - Solutions Notebook\n",
        "\n",
        "This notebook contains complete solutions for all koans. Use this to reference correct answers after attempting the practice notebook.\n",
        "\n",
        "**Note**: These koans are designed to work with the browser-based pandas shim. To run with real PySpark, you'll need a Spark environment.\n",
        "\n",
        "## Categories:\n",
        "- **Koans 1-30**: PySpark Basics and Operations\n",
        "- **Koans 101-110**: Delta Lake\n",
        "- **Koans 201-210**: Unity Catalog\n",
        "- **Koans 301-310**: Pandas API on Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "# For browser-based version, spark is already initialized\n",
        "print(\"✓ Environment ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 1: Creating a DataFrame\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
        "columns = [\"name\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "assert df.count() == 3\n",
        "assert len(df.columns) == 2\n",
        "print(\"✓ Koan 1 complete: DataFrame creation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 2: Selecting Columns\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34, \"NYC\"), (\"Bob\", 45, \"LA\"), (\"Charlie\", 29, \"Chicago\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\n",
        "\n",
        "result = df.select(\"name\", \"city\")\n",
        "\n",
        "assert len(result.columns) == 2\n",
        "assert \"name\" in result.columns\n",
        "assert \"city\" in result.columns\n",
        "print(\"✓ Koan 2 complete: Column selection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 3: Filtering Rows\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29), (\"Diana\", 52)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.filter(col(\"age\") > 35)\n",
        "\n",
        "assert result.count() == 2\n",
        "rows = result.collect()\n",
        "ages = [row[\"age\"] for row in rows]\n",
        "assert all(age > 35 for age in ages)\n",
        "print(\"✓ Koan 3 complete: Row filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 4: Adding Columns\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.withColumn(\"age_in_months\", col(\"age\") * 12)\n",
        "\n",
        "assert result.count() == 3\n",
        "assert len(result.columns) == 3\n",
        "first_row = result.filter(col(\"name\") == \"Alice\").collect()[0]\n",
        "assert first_row[\"age_in_months\"] == 408\n",
        "print(\"✓ Koan 4 complete: Adding columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 5: Grouping and Aggregating\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 5000),\n",
        "    (\"Sales\", \"Bob\", 4500),\n",
        "    (\"Engineering\", \"Charlie\", 6000),\n",
        "    (\"Engineering\", \"Diana\", 6500),\n",
        "    (\"Engineering\", \"Eve\", 5500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"])\n",
        "\n",
        "result = df.groupBy(\"department\").agg(\n",
        "    round(avg(\"salary\"), 2).alias(\"avg_salary\")\n",
        ")\n",
        "\n",
        "assert result.count() == 2\n",
        "eng_row = result.filter(col(\"department\") == \"Engineering\").collect()[0]\n",
        "assert eng_row[\"avg_salary\"] == 6000.0\n",
        "print(\"✓ Koan 5 complete: Grouping and aggregating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 6: Dropping Columns\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34, \"NYC\", \"F\"), (\"Bob\", 45, \"LA\", \"M\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"gender\"])\n",
        "\n",
        "result = df.drop(\"gender\")\n",
        "result2 = df.drop(\"city\", \"gender\")\n",
        "\n",
        "assert \"gender\" not in result.columns\n",
        "assert len(result.columns) == 3\n",
        "assert len(result2.columns) == 2\n",
        "print(\"✓ Koan 6 complete: Dropping columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 7: Distinct Values\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", \"NYC\"), (\"Bob\", \"LA\"), (\"Alice\", \"NYC\"), (\"Charlie\", \"NYC\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"city\"])\n",
        "\n",
        "result = df.distinct()\n",
        "cities = df.select(\"city\").distinct()\n",
        "\n",
        "assert result.count() == 3\n",
        "assert cities.count() == 2\n",
        "print(\"✓ Koan 7 complete: Distinct values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Column Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 9: Renaming Columns\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.withColumnRenamed(\"name\", \"employee_name\")\n",
        "result2 = df.select(col(\"name\").alias(\"full_name\"), col(\"age\"))\n",
        "\n",
        "assert \"employee_name\" in result.columns\n",
        "assert \"name\" not in result.columns\n",
        "assert \"full_name\" in result2.columns\n",
        "print(\"✓ Koan 9 complete: Renaming columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 10: Literal Values\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.withColumn(\"country\", lit(\"USA\"))\n",
        "result2 = df.withColumn(\"bonus\", lit(1000))\n",
        "\n",
        "rows = result.collect()\n",
        "assert all(row[\"country\"] == \"USA\" for row in rows)\n",
        "assert result2.collect()[0][\"bonus\"] == 1000\n",
        "print(\"✓ Koan 10 complete: Literal values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 11: Conditional Logic with when/otherwise\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 17), (\"Diana\", 65)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.withColumn(\n",
        "    \"age_group\",\n",
        "    when(col(\"age\") < 18, \"minor\")\n",
        "    .when(col(\"age\") < 65, \"adult\")\n",
        "    .otherwise(\"senior\")\n",
        ")\n",
        "\n",
        "rows = result.collect()\n",
        "groups = {row[\"name\"]: row[\"age_group\"] for row in rows}\n",
        "\n",
        "assert groups[\"Charlie\"] == \"minor\"\n",
        "assert groups[\"Alice\"] == \"adult\"\n",
        "assert groups[\"Diana\"] == \"senior\"\n",
        "print(\"✓ Koan 11 complete: Conditional logic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 12: Type Casting\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", \"34\"), (\"Bob\", \"45\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age_str\"])\n",
        "\n",
        "result = df.withColumn(\"age\", col(\"age_str\").cast(\"integer\"))\n",
        "result = result.withColumn(\"age_plus_10\", col(\"age\") + 10)\n",
        "\n",
        "result2 = df.withColumn(\"age_float\", col(\"age_str\").cast(\"double\"))\n",
        "\n",
        "rows = result.collect()\n",
        "assert rows[0][\"age_plus_10\"] == 44\n",
        "print(\"✓ Koan 12 complete: Type casting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 13: String Functions - Case\n",
        "# Solution\n",
        "\n",
        "data = [(\"alice smith\",), (\"BOB JONES\",), (\"Charlie Brown\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "result = df.withColumn(\"upper_name\", upper(col(\"name\")))\n",
        "assert result.collect()[0][\"upper_name\"] == \"ALICE SMITH\"\n",
        "\n",
        "result = df.withColumn(\"lower_name\", lower(col(\"name\")))\n",
        "assert result.collect()[1][\"lower_name\"] == \"bob jones\"\n",
        "\n",
        "result = df.withColumn(\"title_name\", initcap(col(\"name\")))\n",
        "assert result.collect()[0][\"title_name\"] == \"Alice Smith\"\n",
        "\n",
        "print(\"✓ Koan 13 complete: String case functions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 14: String Functions - Concatenation\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", \"Smith\"), (\"Bob\", \"Jones\")]\n",
        "df = spark.createDataFrame(data, [\"first\", \"last\"])\n",
        "\n",
        "result = df.withColumn(\"full_name\", concat(col(\"first\"), lit(\" \"), col(\"last\")))\n",
        "assert result.collect()[0][\"full_name\"] == \"Alice Smith\"\n",
        "\n",
        "result2 = df.withColumn(\"full_name\", concat_ws(\" \", col(\"first\"), col(\"last\")))\n",
        "assert result2.collect()[0][\"full_name\"] == \"Alice Smith\"\n",
        "\n",
        "print(\"✓ Koan 14 complete: String concatenation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 15: String Functions - Substring and Length\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\",), (\"Bob\",), (\"Charlotte\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "result = df.withColumn(\"name_length\", length(col(\"name\")))\n",
        "lengths = [row[\"name_length\"] for row in result.collect()]\n",
        "assert lengths == [5, 3, 9]\n",
        "\n",
        "result2 = df.withColumn(\"first_three\", substring(col(\"name\"), 1, 3))\n",
        "firsts = [row[\"first_three\"] for row in result2.collect()]\n",
        "assert firsts == [\"Ali\", \"Bob\", \"Cha\"]\n",
        "\n",
        "print(\"✓ Koan 15 complete: Substring and length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 16: String Functions - Trim and Pad\n",
        "# Solution\n",
        "\n",
        "data = [(\"  Alice  \",), (\"Bob\",), (\" Charlie \",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "result = df.withColumn(\"trimmed\", trim(col(\"name\")))\n",
        "trimmed = [row[\"trimmed\"] for row in result.collect()]\n",
        "assert trimmed == [\"Alice\", \"Bob\", \"Charlie\"]\n",
        "\n",
        "result2 = df.withColumn(\"trimmed\", trim(col(\"name\")))\n",
        "result2 = result2.withColumn(\"padded\", lpad(col(\"trimmed\"), 10, \"*\"))\n",
        "assert result2.collect()[1][\"padded\"] == \"*******Bob\"\n",
        "\n",
        "print(\"✓ Koan 16 complete: Trim and pad functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 17: Grouping and Aggregating\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 5000),\n",
        "    (\"Sales\", \"Bob\", 4500),\n",
        "    (\"Engineering\", \"Charlie\", 6000),\n",
        "    (\"Engineering\", \"Diana\", 6500),\n",
        "    (\"Engineering\", \"Eve\", 5500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"])\n",
        "\n",
        "result = df.groupBy(\"department\").agg(\n",
        "    round(avg(\"salary\"), 2).alias(\"avg_salary\")\n",
        ")\n",
        "\n",
        "assert result.count() == 2\n",
        "eng_row = result.filter(col(\"department\") == \"Engineering\").collect()[0]\n",
        "assert eng_row[\"avg_salary\"] == 6000.0\n",
        "print(\"✓ Koan 17 complete: Grouping and aggregating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 18: Multiple Aggregations\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"Sales\", 5000), (\"Sales\", 4500), (\"Sales\", 6000),\n",
        "    (\"Engineering\", 6000), (\"Engineering\", 6500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"department\", \"salary\"])\n",
        "\n",
        "result = df.groupBy(\"department\").agg(\n",
        "    min(\"salary\").alias(\"min_salary\"),\n",
        "    max(\"salary\").alias(\"max_salary\"),\n",
        "    avg(\"salary\").alias(\"avg_salary\"),\n",
        "    count(\"salary\").alias(\"emp_count\")\n",
        ")\n",
        "\n",
        "sales = result.filter(col(\"department\") == \"Sales\").collect()[0]\n",
        "assert sales[\"min_salary\"] == 4500\n",
        "assert sales[\"max_salary\"] == 6000\n",
        "assert sales[\"emp_count\"] == 3\n",
        "print(\"✓ Koan 18 complete: Multiple aggregations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 19: Aggregate Without Grouping\n",
        "# Solution\n",
        "\n",
        "data = [(100,), (200,), (300,), (400,), (500,)]\n",
        "df = spark.createDataFrame(data, [\"value\"])\n",
        "\n",
        "result = df.agg(sum(col(\"value\")).alias(\"total\"))\n",
        "total = result.collect()[0][\"total\"]\n",
        "assert total == 1500\n",
        "\n",
        "result2 = df.agg(\n",
        "    sum(col(\"value\")).alias(\"total\"),\n",
        "    avg(\"value\").alias(\"average\"),\n",
        "    count(\"value\").alias(\"num_rows\")\n",
        ")\n",
        "\n",
        "row = result2.collect()[0]\n",
        "assert row[\"average\"] == 300.0\n",
        "assert row[\"num_rows\"] == 5\n",
        "print(\"✓ Koan 19 complete: Global aggregations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 20: Inner Join\n",
        "# Solution\n",
        "\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 101),\n",
        "    (2, \"Bob\", 102),\n",
        "    (3, \"Charlie\", 101)\n",
        "], [\"emp_id\", \"name\", \"dept_id\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (101, \"Engineering\"),\n",
        "    (102, \"Sales\"),\n",
        "    (103, \"Marketing\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "result = employees.join(departments, \"dept_id\", \"inner\")\n",
        "\n",
        "assert result.count() == 3\n",
        "assert \"name\" in result.columns\n",
        "assert \"dept_name\" in result.columns\n",
        "alice = result.filter(col(\"name\") == \"Alice\").collect()[0]\n",
        "assert alice[\"dept_name\"] == \"Engineering\"\n",
        "print(\"✓ Koan 20 complete: Inner joins\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 21: Left Outer Join\n",
        "# Solution\n",
        "\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 101),\n",
        "    (2, \"Bob\", 102),\n",
        "    (3, \"Charlie\", 999)\n",
        "], [\"emp_id\", \"name\", \"dept_id\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (101, \"Engineering\"),\n",
        "    (102, \"Sales\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "result = employees.join(departments, \"dept_id\", \"left\")\n",
        "\n",
        "assert result.count() == 3\n",
        "charlie = result.filter(col(\"name\") == \"Charlie\").collect()[0]\n",
        "assert charlie[\"dept_name\"] is None\n",
        "print(\"✓ Koan 21 complete: Left outer joins\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 22: Join on Multiple Columns\n",
        "# Solution\n",
        "\n",
        "orders = spark.createDataFrame([\n",
        "    (\"2024\", \"Q1\", \"Alice\", 100),\n",
        "    (\"2024\", \"Q2\", \"Alice\", 150),\n",
        "    (\"2024\", \"Q1\", \"Bob\", 200)\n",
        "], [\"year\", \"quarter\", \"rep\", \"amount\"])\n",
        "\n",
        "targets = spark.createDataFrame([\n",
        "    (\"2024\", \"Q1\", 120),\n",
        "    (\"2024\", \"Q2\", 140)\n",
        "], [\"year\", \"quarter\", \"target\"])\n",
        "\n",
        "result = orders.join(targets, [\"year\", \"quarter\"], \"inner\")\n",
        "\n",
        "assert result.count() == 3\n",
        "alice_q1 = result.filter((col(\"rep\") == \"Alice\") & (col(\"quarter\") == \"Q1\")).collect()[0]\n",
        "assert alice_q1[\"target\"] == 120\n",
        "print(\"✓ Koan 22 complete: Multi-column joins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 23: Window Functions - Running Total\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"2024-01-01\", 100),\n",
        "    (\"2024-01-02\", 150),\n",
        "    (\"2024-01-03\", 200),\n",
        "    (\"2024-01-04\", 175)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"date\", \"sales\"])\n",
        "\n",
        "window_spec = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "result = df.withColumn(\"running_total\", sum(col(\"sales\")).over(window_spec))\n",
        "\n",
        "rows = result.orderBy(\"date\").collect()\n",
        "assert rows[0][\"running_total\"] == 100\n",
        "assert rows[1][\"running_total\"] == 250\n",
        "assert rows[3][\"running_total\"] == 625\n",
        "print(\"✓ Koan 23 complete: Window running totals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 24: Window Functions - Row Number\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"Sales\", \"Alice\", 5000),\n",
        "    (\"Sales\", \"Bob\", 5500),\n",
        "    (\"Engineering\", \"Charlie\", 6000),\n",
        "    (\"Engineering\", \"Diana\", 6500),\n",
        "    (\"Engineering\", \"Eve\", 5500)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"dept\", \"name\", \"salary\"])\n",
        "\n",
        "window_spec = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
        "result = df.withColumn(\"rank\", row_number().over(window_spec))\n",
        "\n",
        "eng = result.filter(col(\"dept\") == \"Engineering\").orderBy(\"rank\").collect()\n",
        "assert eng[0][\"name\"] == \"Diana\"\n",
        "assert eng[0][\"rank\"] == 1\n",
        "assert eng[1][\"name\"] == \"Charlie\"\n",
        "print(\"✓ Koan 24 complete: Row number window functions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 25: Window Functions - Lag and Lead\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"2024-01-01\", 100),\n",
        "    (\"2024-01-02\", 150),\n",
        "    (\"2024-01-03\", 120),\n",
        "    (\"2024-01-04\", 200)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"date\", \"price\"])\n",
        "\n",
        "window_spec = Window.orderBy(\"date\")\n",
        "\n",
        "result = df.withColumn(\"prev_price\", lag(\"price\", 1).over(window_spec))\n",
        "result = result.withColumn(\"change\", col(\"price\") - col(\"prev_price\"))\n",
        "\n",
        "rows = result.orderBy(\"date\").collect()\n",
        "assert rows[0][\"prev_price\"] is None\n",
        "assert rows[1][\"prev_price\"] == 100\n",
        "assert rows[1][\"change\"] == 50\n",
        "\n",
        "result2 = df.withColumn(\"next_price\", lead(\"price\", 1).over(window_spec))\n",
        "rows2 = result2.orderBy(\"date\").collect()\n",
        "assert rows2[0][\"next_price\"] == 150\n",
        "print(\"✓ Koan 25 complete: Lag and lead window functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Null Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 26: Handling Nulls - Detection\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", None), (\"Charlie\", 29), (None, 45)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.filter(col(\"age\").isNotNull())\n",
        "assert result.count() == 3\n",
        "\n",
        "nulls = df.filter(col(\"age\").isNull())\n",
        "assert nulls.count() == 1\n",
        "\n",
        "null_names = df.filter(col(\"name\").isNull())\n",
        "assert null_names.count() == 1\n",
        "print(\"✓ Koan 26 complete: Null detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 27: Handling Nulls - Fill and Drop\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", None), (None, 29), (\"Diana\", None)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "result = df.fillna(0, subset=[\"age\"])\n",
        "ages = [row[\"age\"] for row in result.collect()]\n",
        "assert None not in ages\n",
        "assert ages.count(0) == 2\n",
        "\n",
        "result2 = df.fillna(\"Unknown\", subset=[\"name\"])\n",
        "names = [row[\"name\"] for row in result2.collect()]\n",
        "assert \"Unknown\" in names\n",
        "\n",
        "result3 = df.dropna()\n",
        "assert result3.count() == 1\n",
        "print(\"✓ Koan 27 complete: Null handling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 28: Union DataFrames\n",
        "# Solution\n",
        "\n",
        "df1 = spark.createDataFrame([(\"Alice\", 34), (\"Bob\", 45)], [\"name\", \"age\"])\n",
        "df2 = spark.createDataFrame([(\"Charlie\", 29), (\"Diana\", 52)], [\"name\", \"age\"])\n",
        "\n",
        "result = df1.union(df2)\n",
        "\n",
        "assert result.count() == 4\n",
        "names = [row[\"name\"] for row in result.collect()]\n",
        "assert \"Alice\" in names and \"Charlie\" in names\n",
        "print(\"✓ Koan 28 complete: Union DataFrames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 29: Explode Arrays\n",
        "# Solution\n",
        "\n",
        "data = [(\"Alice\", \"python,sql,spark\"), (\"Bob\", \"java,scala\")]\n",
        "df = spark.createDataFrame(data, [\"name\", \"skills_str\"])\n",
        "\n",
        "df = df.withColumn(\"skills\", split(col(\"skills_str\"), \",\"))\n",
        "result = df.select(\"name\", explode(col(\"skills\")).alias(\"skill\"))\n",
        "\n",
        "assert result.count() == 5\n",
        "alice_skills = [row[\"skill\"] for row in result.filter(col(\"name\") == \"Alice\").collect()]\n",
        "assert len(alice_skills) == 3\n",
        "assert \"spark\" in alice_skills\n",
        "print(\"✓ Koan 29 complete: Explode arrays\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Koan 30: Pivot Tables\n",
        "# Solution\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", \"Q1\", 100), (\"Alice\", \"Q2\", 150),\n",
        "    (\"Bob\", \"Q1\", 200), (\"Bob\", \"Q2\", 180)\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"name\", \"quarter\", \"sales\"])\n",
        "\n",
        "result = df.groupBy(\"name\").pivot(\"quarter\").agg(sum(col(\"sales\")))\n",
        "\n",
        "assert \"Q1\" in result.columns\n",
        "assert \"Q2\" in result.columns\n",
        "alice = result.filter(col(\"name\") == \"Alice\").collect()[0]\n",
        "assert alice[\"Q1\"] == 100\n",
        "assert alice[\"Q2\"] == 150\n",
        "print(\"✓ Koan 30 complete: Pivot tables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've completed 30 core PySpark koans covering:\n",
        "- DataFrame basics (create, select, filter, add/drop columns)\n",
        "- Aggregations and grouping\n",
        "- String functions and type casting\n",
        "- Joins and window functions\n",
        "- Null handling\n",
        "- Advanced operations (union, explode, pivot)\n",
        "\n",
        "Additional koans for Delta Lake (101-110), Unity Catalog (201-210), and Pandas API on Spark (301-310) require specialized environments and are documented in the full solutions notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
